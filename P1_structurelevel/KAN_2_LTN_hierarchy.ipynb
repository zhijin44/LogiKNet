{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "**KAN: prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Data prepared.\n",
      "Train set: (torch.Size([89918, 20]), torch.Size([89918, 4]))\n",
      "Test set: (torch.Size([9991, 20]), torch.Size([9991, 4]))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', \n",
    "    # 'Drate',\n",
    "    # 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number',\n",
    "    # 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "    # 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
    "    # 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', \n",
    "    'IPv','LLC', \n",
    "    'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
    "    'Magnitue', 'Radius', 'Covariance',\n",
    "    'Variance', 'Weight'\n",
    "]\n",
    "\n",
    "Y_columns = ['label_L1']\n",
    "\n",
    "label_L1_mapping = {\"MQTT\": 0, \"Benign\": 1, \"Recon\": 2, \"ARP_Spoofing\": 3}\n",
    "label_L2_mapping = {\"MQTT-DDoS-Connect_Flood\": 4, \"MQTT-DDoS-Publish_Flood\": 5, \n",
    "                    \"MQTT-DoS-Connect_Flood\": 6, \"MQTT-DoS-Publish_Flood\": 7,\n",
    "                    \"MQTT-Malformed_Data\": 8, \"benign\": 9, \n",
    "                    \"Recon-OS_Scan\": 10, \"Recon-Port_Scan\": 11,\n",
    "                    \"arp_spoofing\": 12}\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/home/zyang44/Github/baseline_cicIOT/CIC_IoMT/19classes/filtered_train_l_4_11.csv')\n",
    "df['label_L1'] = df['label_L1'].map(label_L1_mapping)\n",
    "df['label_L2'] = df['label_L2'].map(label_L1_mapping)\n",
    "\n",
    "# Shuffle the dataframe before splitting into training and test sets\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "# 90% as training set and 10% as test set\n",
    "train_size = int(len(df) * 0.9)\n",
    "train_df, test_df = df.iloc[:train_size, :], df.iloc[train_size:, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[X_columns])\n",
    "test_X_scaled = scaler.transform(test_df[X_columns])\n",
    "train_y = train_df[Y_columns].values.ravel()\n",
    "test_y = test_df[Y_columns].values.ravel()\n",
    "\n",
    "# take Y_columns as the label, and transfering to one-hot coded\n",
    "dataset = {\n",
    "    'train_input': torch.tensor(train_X_scaled, dtype=torch.float32, device=device),\n",
    "    'train_label': F.one_hot(torch.tensor(train_y, dtype=torch.long, device=device), num_classes=4),\n",
    "    'test_input': torch.tensor(test_X_scaled, dtype=torch.float32, device=device),\n",
    "    'test_label': F.one_hot(torch.tensor(test_y, dtype=torch.long, device=device), num_classes=4)\n",
    "}\n",
    "print(\"Data prepared.\",\n",
    "      f\"Train set: {dataset['train_input'].shape, dataset['train_label'].shape}\",\n",
    "      f\"Test set: {dataset['test_input'].shape, dataset['test_label'].shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build myKAN**\n",
    "\n",
    "A wrapper function, to only get the logits output by the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiKANModel(nn.Module):\n",
    "    def __init__(self, kan):\n",
    "        \"\"\"\n",
    "        Wrap an already built MultKAN instance.\n",
    "        Args:\n",
    "            kan: a MultKAN model (which has attributes such as act_fun, symbolic_fun, node_bias, node_scale,\n",
    "                 subnode_bias, subnode_scale, depth, width, mult_homo, mult_arity, input_id, symbolic_enabled, etc.)\n",
    "        \"\"\"\n",
    "        super(MultiKANModel, self).__init__()\n",
    "        self.kan = kan\n",
    "\n",
    "    def forward(self, x, training=False, singularity_avoiding=False, y_th=10.):\n",
    "        # Select input features according to input_id\n",
    "        x = x[:, self.kan.input_id.long()]\n",
    "        # Loop through each layer\n",
    "        for l in range(self.kan.depth):\n",
    "            # Get outputs from the numerical branch (KANLayer) of current layer\n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.kan.act_fun[l](x)\n",
    "            # Get output from the symbolic branch if enabled\n",
    "            if self.kan.symbolic_enabled:\n",
    "                x_symbolic, postacts_symbolic = self.kan.symbolic_fun[l](x, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            else:\n",
    "                x_symbolic = 0.\n",
    "            # Sum the numerical and symbolic outputs\n",
    "            x = x_numerical + x_symbolic\n",
    "\n",
    "            # Subnode affine transformation\n",
    "            x = self.kan.subnode_scale[l][None, :] * x + self.kan.subnode_bias[l][None, :]\n",
    "\n",
    "            # Process multiplication nodes\n",
    "            dim_sum = self.kan.width[l+1][0]\n",
    "            dim_mult = self.kan.width[l+1][1]\n",
    "            if dim_mult > 0:\n",
    "                if self.kan.mult_homo:\n",
    "                    for i in range(self.kan.mult_arity-1):\n",
    "                        if i == 0:\n",
    "                            x_mult = x[:, dim_sum::self.kan.mult_arity] * x[:, dim_sum+1::self.kan.mult_arity]\n",
    "                        else:\n",
    "                            x_mult = x_mult * x[:, dim_sum+i+1::self.kan.mult_arity]\n",
    "                else:\n",
    "                    for j in range(dim_mult):\n",
    "                        acml_id = dim_sum + int(np.sum(self.kan.mult_arity[l+1][:j]))\n",
    "                        for i in range(self.kan.mult_arity[l+1][j]-1):\n",
    "                            if i == 0:\n",
    "                                x_mult_j = x[:, [acml_id]] * x[:, [acml_id+1]]\n",
    "                            else:\n",
    "                                x_mult_j = x_mult_j * x[:, [acml_id+i+1]]\n",
    "                        if j == 0:\n",
    "                            x_mult = x_mult_j\n",
    "                        else:\n",
    "                            x_mult = torch.cat([x_mult, x_mult_j], dim=1)\n",
    "                # Concatenate sum and mult parts\n",
    "                x = torch.cat([x[:, :dim_sum], x_mult], dim=1)\n",
    "\n",
    "            # Node affine transformation\n",
    "            x = self.kan.node_scale[l][None, :] * x + self.kan.node_bias[l][None, :]\n",
    "\n",
    "        # Final x corresponds to the logits output of the whole model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LTN Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import ltn.fuzzy_ops\n",
    "\n",
    "# define the connectives, quantifiers, and the SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())   # And = ltn.Connective(custom_fuzzy_ops.AndProd())\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "# define ltn constants\n",
    "l_MQTT = ltn.Constant(torch.tensor([1, 0, 0, 0]))\n",
    "l_Benign = ltn.Constant(torch.tensor([0, 1, 0, 0]))\n",
    "l_Recon = ltn.Constant(torch.tensor([0, 0, 1, 0]))\n",
    "l_ARP_Spoofing = ltn.Constant(torch.tensor([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from utils import MLP, LogitsToPredicate, DataLoader, DataLoaderMulti\n",
    "from kan import KAN\n",
    "\n",
    "# Define the MLP predicate\n",
    "mlp = MLP(layer_sizes=(20, 64, 4)).to(device)\n",
    "P_mlp = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# Define myKAN predicate\n",
    "kan = KAN(width=[20, 10, 4], grid=5, k=3, seed=42, device=device)\n",
    "mykan = MultiKANModel(kan)\n",
    "P_kan = ltn.Predicate(LogitsToPredicate(mykan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN effect**\n",
    "\n",
    "LTN(kan) v.s LTN(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | MLP (loss/acc/sat): 0.616/0.561/0.384(0.389) | KAN (loss/acc/sat): 0.694/0.675/0.306(0.308)\n",
      "Epoch 1 | MLP (loss/acc/sat): 0.613/0.561/0.387(0.392) | KAN (loss/acc/sat): 0.692/0.677/0.308(0.310)\n",
      "Epoch 2 | MLP (loss/acc/sat): 0.610/0.563/0.390(0.395) | KAN (loss/acc/sat): 0.690/0.680/0.310(0.312)\n",
      "Epoch 3 | MLP (loss/acc/sat): 0.608/0.564/0.392(0.398) | KAN (loss/acc/sat): 0.689/0.680/0.311(0.314)\n",
      "Epoch 4 | MLP (loss/acc/sat): 0.605/0.564/0.395(0.401) | KAN (loss/acc/sat): 0.687/0.677/0.313(0.316)\n",
      "Epoch 5 | MLP (loss/acc/sat): 0.602/0.565/0.398(0.403) | KAN (loss/acc/sat): 0.685/0.676/0.315(0.317)\n",
      "Epoch 6 | MLP (loss/acc/sat): 0.600/0.564/0.400(0.406) | KAN (loss/acc/sat): 0.683/0.676/0.317(0.319)\n",
      "Epoch 7 | MLP (loss/acc/sat): 0.597/0.565/0.403(0.408) | KAN (loss/acc/sat): 0.682/0.675/0.318(0.321)\n",
      "Epoch 8 | MLP (loss/acc/sat): 0.595/0.566/0.405(0.411) | KAN (loss/acc/sat): 0.680/0.675/0.320(0.323)\n",
      "Epoch 9 | MLP (loss/acc/sat): 0.593/0.567/0.407(0.413) | KAN (loss/acc/sat): 0.678/0.672/0.322(0.324)\n",
      "Epoch 10 | MLP (loss/acc/sat): 0.590/0.567/0.410(0.415) | KAN (loss/acc/sat): 0.677/0.668/0.323(0.326)\n",
      "Epoch 11 | MLP (loss/acc/sat): 0.588/0.569/0.412(0.417) | KAN (loss/acc/sat): 0.675/0.666/0.325(0.328)\n",
      "Epoch 12 | MLP (loss/acc/sat): 0.586/0.571/0.414(0.420) | KAN (loss/acc/sat): 0.673/0.666/0.327(0.330)\n",
      "Epoch 13 | MLP (loss/acc/sat): 0.584/0.574/0.416(0.422) | KAN (loss/acc/sat): 0.671/0.665/0.329(0.331)\n",
      "Epoch 14 | MLP (loss/acc/sat): 0.582/0.575/0.418(0.424) | KAN (loss/acc/sat): 0.670/0.665/0.330(0.333)\n",
      "Epoch 15 | MLP (loss/acc/sat): 0.580/0.576/0.420(0.426) | KAN (loss/acc/sat): 0.668/0.665/0.332(0.335)\n",
      "Epoch 16 | MLP (loss/acc/sat): 0.578/0.577/0.422(0.428) | KAN (loss/acc/sat): 0.666/0.666/0.334(0.337)\n",
      "Epoch 17 | MLP (loss/acc/sat): 0.576/0.579/0.424(0.430) | KAN (loss/acc/sat): 0.664/0.667/0.336(0.339)\n",
      "Epoch 18 | MLP (loss/acc/sat): 0.574/0.580/0.426(0.432) | KAN (loss/acc/sat): 0.663/0.668/0.337(0.340)\n",
      "Epoch 19 | MLP (loss/acc/sat): 0.572/0.582/0.428(0.434) | KAN (loss/acc/sat): 0.661/0.668/0.339(0.342)\n",
      "Epoch 20 | MLP (loss/acc/sat): 0.570/0.584/0.430(0.436) | KAN (loss/acc/sat): 0.659/0.669/0.341(0.344)\n",
      "Epoch 21 | MLP (loss/acc/sat): 0.568/0.585/0.432(0.438) | KAN (loss/acc/sat): 0.657/0.669/0.343(0.346)\n",
      "Epoch 22 | MLP (loss/acc/sat): 0.566/0.587/0.434(0.440) | KAN (loss/acc/sat): 0.655/0.671/0.345(0.348)\n",
      "Epoch 23 | MLP (loss/acc/sat): 0.564/0.589/0.436(0.442) | KAN (loss/acc/sat): 0.654/0.672/0.346(0.350)\n",
      "Epoch 24 | MLP (loss/acc/sat): 0.562/0.593/0.438(0.443) | KAN (loss/acc/sat): 0.652/0.672/0.348(0.352)\n",
      "Epoch 25 | MLP (loss/acc/sat): 0.560/0.595/0.440(0.445) | KAN (loss/acc/sat): 0.650/0.673/0.350(0.354)\n",
      "Epoch 26 | MLP (loss/acc/sat): 0.559/0.596/0.441(0.447) | KAN (loss/acc/sat): 0.648/0.674/0.352(0.356)\n",
      "Epoch 27 | MLP (loss/acc/sat): 0.557/0.599/0.443(0.449) | KAN (loss/acc/sat): 0.646/0.674/0.354(0.358)\n",
      "Epoch 28 | MLP (loss/acc/sat): 0.555/0.602/0.445(0.451) | KAN (loss/acc/sat): 0.644/0.674/0.356(0.360)\n",
      "Epoch 29 | MLP (loss/acc/sat): 0.553/0.607/0.447(0.453) | KAN (loss/acc/sat): 0.642/0.675/0.358(0.362)\n",
      "Epoch 30 | MLP (loss/acc/sat): 0.551/0.615/0.449(0.454) | KAN (loss/acc/sat): 0.640/0.676/0.360(0.364)\n",
      "Epoch 31 | MLP (loss/acc/sat): 0.550/0.626/0.450(0.456) | KAN (loss/acc/sat): 0.638/0.677/0.362(0.366)\n",
      "Epoch 32 | MLP (loss/acc/sat): 0.548/0.645/0.452(0.458) | KAN (loss/acc/sat): 0.636/0.678/0.364(0.368)\n",
      "Epoch 33 | MLP (loss/acc/sat): 0.546/0.679/0.454(0.459) | KAN (loss/acc/sat): 0.634/0.679/0.366(0.370)\n",
      "Epoch 34 | MLP (loss/acc/sat): 0.545/0.720/0.455(0.461) | KAN (loss/acc/sat): 0.632/0.680/0.368(0.372)\n",
      "Epoch 35 | MLP (loss/acc/sat): 0.543/0.758/0.457(0.462) | KAN (loss/acc/sat): 0.630/0.682/0.370(0.375)\n",
      "Epoch 36 | MLP (loss/acc/sat): 0.542/0.774/0.458(0.464) | KAN (loss/acc/sat): 0.627/0.682/0.373(0.377)\n",
      "Epoch 37 | MLP (loss/acc/sat): 0.540/0.778/0.460(0.465) | KAN (loss/acc/sat): 0.625/0.683/0.375(0.379)\n",
      "Epoch 38 | MLP (loss/acc/sat): 0.539/0.780/0.461(0.467) | KAN (loss/acc/sat): 0.623/0.684/0.377(0.381)\n",
      "Epoch 39 | MLP (loss/acc/sat): 0.537/0.780/0.463(0.468) | KAN (loss/acc/sat): 0.621/0.685/0.379(0.384)\n",
      "Epoch 40 | MLP (loss/acc/sat): 0.536/0.781/0.464(0.470) | KAN (loss/acc/sat): 0.619/0.687/0.381(0.386)\n",
      "Epoch 41 | MLP (loss/acc/sat): 0.534/0.782/0.466(0.471) | KAN (loss/acc/sat): 0.616/0.688/0.384(0.388)\n",
      "Epoch 42 | MLP (loss/acc/sat): 0.533/0.782/0.467(0.473) | KAN (loss/acc/sat): 0.614/0.690/0.386(0.391)\n",
      "Epoch 43 | MLP (loss/acc/sat): 0.531/0.782/0.469(0.474) | KAN (loss/acc/sat): 0.612/0.694/0.388(0.393)\n",
      "Epoch 44 | MLP (loss/acc/sat): 0.530/0.783/0.470(0.475) | KAN (loss/acc/sat): 0.609/0.696/0.391(0.395)\n",
      "Epoch 45 | MLP (loss/acc/sat): 0.529/0.783/0.471(0.477) | KAN (loss/acc/sat): 0.607/0.698/0.393(0.398)\n",
      "Epoch 46 | MLP (loss/acc/sat): 0.527/0.783/0.473(0.478) | KAN (loss/acc/sat): 0.605/0.701/0.395(0.400)\n",
      "Epoch 47 | MLP (loss/acc/sat): 0.526/0.783/0.474(0.479) | KAN (loss/acc/sat): 0.602/0.702/0.398(0.403)\n",
      "Epoch 48 | MLP (loss/acc/sat): 0.525/0.784/0.475(0.481) | KAN (loss/acc/sat): 0.600/0.703/0.400(0.405)\n",
      "Epoch 49 | MLP (loss/acc/sat): 0.523/0.784/0.477(0.482) | KAN (loss/acc/sat): 0.597/0.705/0.403(0.408)\n",
      "Epoch 50 | MLP (loss/acc/sat): 0.522/0.784/0.478(0.483) | KAN (loss/acc/sat): 0.595/0.707/0.405(0.410)\n",
      "Epoch 51 | MLP (loss/acc/sat): 0.521/0.784/0.479(0.485) | KAN (loss/acc/sat): 0.592/0.708/0.408(0.413)\n",
      "Epoch 52 | MLP (loss/acc/sat): 0.520/0.785/0.480(0.486) | KAN (loss/acc/sat): 0.590/0.710/0.410(0.415)\n",
      "Epoch 53 | MLP (loss/acc/sat): 0.518/0.785/0.482(0.487) | KAN (loss/acc/sat): 0.587/0.713/0.413(0.418)\n",
      "Epoch 54 | MLP (loss/acc/sat): 0.517/0.786/0.483(0.488) | KAN (loss/acc/sat): 0.585/0.717/0.415(0.421)\n",
      "Epoch 55 | MLP (loss/acc/sat): 0.516/0.786/0.484(0.489) | KAN (loss/acc/sat): 0.582/0.721/0.418(0.423)\n",
      "Epoch 56 | MLP (loss/acc/sat): 0.515/0.786/0.485(0.491) | KAN (loss/acc/sat): 0.580/0.726/0.420(0.426)\n",
      "Epoch 57 | MLP (loss/acc/sat): 0.514/0.786/0.486(0.492) | KAN (loss/acc/sat): 0.577/0.732/0.423(0.429)\n",
      "Epoch 58 | MLP (loss/acc/sat): 0.512/0.787/0.488(0.493) | KAN (loss/acc/sat): 0.574/0.739/0.426(0.431)\n",
      "Epoch 59 | MLP (loss/acc/sat): 0.511/0.787/0.489(0.494) | KAN (loss/acc/sat): 0.572/0.746/0.428(0.434)\n",
      "Epoch 60 | MLP (loss/acc/sat): 0.510/0.788/0.490(0.495) | KAN (loss/acc/sat): 0.569/0.752/0.431(0.437)\n",
      "Epoch 61 | MLP (loss/acc/sat): 0.509/0.788/0.491(0.496) | KAN (loss/acc/sat): 0.566/0.758/0.434(0.439)\n",
      "Epoch 62 | MLP (loss/acc/sat): 0.508/0.788/0.492(0.498) | KAN (loss/acc/sat): 0.564/0.764/0.436(0.442)\n",
      "Epoch 63 | MLP (loss/acc/sat): 0.507/0.788/0.493(0.499) | KAN (loss/acc/sat): 0.561/0.771/0.439(0.445)\n",
      "Epoch 64 | MLP (loss/acc/sat): 0.506/0.789/0.494(0.500) | KAN (loss/acc/sat): 0.558/0.775/0.442(0.448)\n",
      "Epoch 65 | MLP (loss/acc/sat): 0.504/0.789/0.496(0.501) | KAN (loss/acc/sat): 0.556/0.780/0.444(0.451)\n",
      "Epoch 66 | MLP (loss/acc/sat): 0.503/0.789/0.497(0.502) | KAN (loss/acc/sat): 0.553/0.784/0.447(0.453)\n",
      "Epoch 67 | MLP (loss/acc/sat): 0.502/0.790/0.498(0.503) | KAN (loss/acc/sat): 0.550/0.787/0.450(0.456)\n",
      "Epoch 68 | MLP (loss/acc/sat): 0.501/0.790/0.499(0.504) | KAN (loss/acc/sat): 0.547/0.791/0.453(0.459)\n",
      "Epoch 69 | MLP (loss/acc/sat): 0.500/0.790/0.500(0.505) | KAN (loss/acc/sat): 0.545/0.794/0.455(0.462)\n",
      "Epoch 70 | MLP (loss/acc/sat): 0.499/0.790/0.501(0.506) | KAN (loss/acc/sat): 0.542/0.795/0.458(0.464)\n",
      "Epoch 71 | MLP (loss/acc/sat): 0.498/0.790/0.502(0.507) | KAN (loss/acc/sat): 0.539/0.796/0.461(0.467)\n",
      "Epoch 72 | MLP (loss/acc/sat): 0.497/0.790/0.503(0.509) | KAN (loss/acc/sat): 0.536/0.796/0.464(0.470)\n",
      "Epoch 73 | MLP (loss/acc/sat): 0.496/0.790/0.504(0.510) | KAN (loss/acc/sat): 0.534/0.797/0.466(0.473)\n",
      "Epoch 74 | MLP (loss/acc/sat): 0.495/0.790/0.505(0.511) | KAN (loss/acc/sat): 0.531/0.797/0.469(0.476)\n",
      "Epoch 75 | MLP (loss/acc/sat): 0.494/0.791/0.506(0.512) | KAN (loss/acc/sat): 0.528/0.798/0.472(0.478)\n",
      "Epoch 76 | MLP (loss/acc/sat): 0.493/0.791/0.507(0.513) | KAN (loss/acc/sat): 0.526/0.799/0.474(0.481)\n",
      "Epoch 77 | MLP (loss/acc/sat): 0.492/0.791/0.508(0.514) | KAN (loss/acc/sat): 0.523/0.800/0.477(0.484)\n",
      "Epoch 78 | MLP (loss/acc/sat): 0.491/0.792/0.509(0.515) | KAN (loss/acc/sat): 0.520/0.801/0.480(0.486)\n",
      "Epoch 79 | MLP (loss/acc/sat): 0.490/0.792/0.510(0.516) | KAN (loss/acc/sat): 0.518/0.803/0.482(0.489)\n",
      "Epoch 80 | MLP (loss/acc/sat): 0.489/0.792/0.511(0.517) | KAN (loss/acc/sat): 0.515/0.805/0.485(0.492)\n",
      "Epoch 81 | MLP (loss/acc/sat): 0.488/0.792/0.512(0.518) | KAN (loss/acc/sat): 0.513/0.805/0.487(0.494)\n",
      "Epoch 82 | MLP (loss/acc/sat): 0.487/0.792/0.513(0.519) | KAN (loss/acc/sat): 0.510/0.806/0.490(0.497)\n",
      "Epoch 83 | MLP (loss/acc/sat): 0.486/0.793/0.514(0.519) | KAN (loss/acc/sat): 0.507/0.807/0.493(0.499)\n",
      "Epoch 84 | MLP (loss/acc/sat): 0.485/0.793/0.515(0.520) | KAN (loss/acc/sat): 0.505/0.807/0.495(0.502)\n",
      "Epoch 85 | MLP (loss/acc/sat): 0.484/0.793/0.516(0.521) | KAN (loss/acc/sat): 0.503/0.808/0.497(0.504)\n",
      "Epoch 86 | MLP (loss/acc/sat): 0.483/0.793/0.517(0.522) | KAN (loss/acc/sat): 0.500/0.809/0.500(0.507)\n",
      "Epoch 87 | MLP (loss/acc/sat): 0.482/0.794/0.518(0.523) | KAN (loss/acc/sat): 0.498/0.809/0.502(0.509)\n",
      "Epoch 88 | MLP (loss/acc/sat): 0.481/0.794/0.519(0.524) | KAN (loss/acc/sat): 0.496/0.810/0.504(0.511)\n",
      "Epoch 89 | MLP (loss/acc/sat): 0.480/0.795/0.520(0.525) | KAN (loss/acc/sat): 0.493/0.811/0.507(0.513)\n",
      "Epoch 90 | MLP (loss/acc/sat): 0.479/0.796/0.521(0.526) | KAN (loss/acc/sat): 0.491/0.811/0.509(0.516)\n",
      "Epoch 91 | MLP (loss/acc/sat): 0.478/0.796/0.522(0.527) | KAN (loss/acc/sat): 0.489/0.812/0.511(0.518)\n",
      "Epoch 92 | MLP (loss/acc/sat): 0.477/0.796/0.523(0.528) | KAN (loss/acc/sat): 0.487/0.812/0.513(0.520)\n",
      "Epoch 93 | MLP (loss/acc/sat): 0.477/0.796/0.523(0.528) | KAN (loss/acc/sat): 0.485/0.813/0.515(0.522)\n",
      "Epoch 94 | MLP (loss/acc/sat): 0.476/0.797/0.524(0.529) | KAN (loss/acc/sat): 0.483/0.814/0.517(0.524)\n",
      "Epoch 95 | MLP (loss/acc/sat): 0.475/0.797/0.525(0.530) | KAN (loss/acc/sat): 0.480/0.815/0.520(0.526)\n",
      "Epoch 96 | MLP (loss/acc/sat): 0.474/0.797/0.526(0.531) | KAN (loss/acc/sat): 0.479/0.815/0.521(0.528)\n",
      "Epoch 97 | MLP (loss/acc/sat): 0.473/0.798/0.527(0.532) | KAN (loss/acc/sat): 0.477/0.816/0.523(0.530)\n",
      "Epoch 98 | MLP (loss/acc/sat): 0.472/0.798/0.528(0.532) | KAN (loss/acc/sat): 0.475/0.816/0.525(0.532)\n",
      "Epoch 99 | MLP (loss/acc/sat): 0.472/0.798/0.528(0.533) | KAN (loss/acc/sat): 0.473/0.816/0.527(0.534)\n",
      "Epoch 100 | MLP (loss/acc/sat): 0.471/0.798/0.529(0.534) | KAN (loss/acc/sat): 0.471/0.816/0.529(0.535)\n",
      "Epoch 101 | MLP (loss/acc/sat): 0.470/0.799/0.530(0.535) | KAN (loss/acc/sat): 0.470/0.817/0.530(0.537)\n",
      "Epoch 102 | MLP (loss/acc/sat): 0.469/0.799/0.531(0.536) | KAN (loss/acc/sat): 0.468/0.817/0.532(0.539)\n",
      "Epoch 103 | MLP (loss/acc/sat): 0.469/0.799/0.531(0.536) | KAN (loss/acc/sat): 0.466/0.817/0.534(0.540)\n",
      "Epoch 104 | MLP (loss/acc/sat): 0.468/0.800/0.532(0.537) | KAN (loss/acc/sat): 0.465/0.818/0.535(0.542)\n",
      "Epoch 105 | MLP (loss/acc/sat): 0.467/0.800/0.533(0.538) | KAN (loss/acc/sat): 0.463/0.819/0.537(0.543)\n",
      "Epoch 106 | MLP (loss/acc/sat): 0.466/0.800/0.534(0.538) | KAN (loss/acc/sat): 0.462/0.819/0.538(0.545)\n",
      "Epoch 107 | MLP (loss/acc/sat): 0.466/0.801/0.534(0.539) | KAN (loss/acc/sat): 0.460/0.820/0.540(0.546)\n",
      "Epoch 108 | MLP (loss/acc/sat): 0.465/0.801/0.535(0.540) | KAN (loss/acc/sat): 0.459/0.820/0.541(0.547)\n",
      "Epoch 109 | MLP (loss/acc/sat): 0.464/0.801/0.536(0.540) | KAN (loss/acc/sat): 0.458/0.820/0.542(0.549)\n",
      "Epoch 110 | MLP (loss/acc/sat): 0.464/0.801/0.536(0.541) | KAN (loss/acc/sat): 0.456/0.820/0.544(0.550)\n",
      "Epoch 111 | MLP (loss/acc/sat): 0.463/0.802/0.537(0.542) | KAN (loss/acc/sat): 0.455/0.820/0.545(0.551)\n",
      "Epoch 112 | MLP (loss/acc/sat): 0.463/0.802/0.537(0.542) | KAN (loss/acc/sat): 0.454/0.820/0.546(0.552)\n",
      "Epoch 113 | MLP (loss/acc/sat): 0.462/0.802/0.538(0.543) | KAN (loss/acc/sat): 0.453/0.821/0.547(0.553)\n",
      "Epoch 114 | MLP (loss/acc/sat): 0.461/0.803/0.539(0.543) | KAN (loss/acc/sat): 0.452/0.821/0.548(0.555)\n",
      "Epoch 115 | MLP (loss/acc/sat): 0.461/0.803/0.539(0.544) | KAN (loss/acc/sat): 0.451/0.821/0.549(0.556)\n",
      "Epoch 116 | MLP (loss/acc/sat): 0.460/0.803/0.540(0.544) | KAN (loss/acc/sat): 0.450/0.821/0.550(0.557)\n",
      "Epoch 117 | MLP (loss/acc/sat): 0.460/0.804/0.540(0.545) | KAN (loss/acc/sat): 0.449/0.821/0.551(0.558)\n",
      "Epoch 118 | MLP (loss/acc/sat): 0.459/0.804/0.541(0.545) | KAN (loss/acc/sat): 0.448/0.821/0.552(0.559)\n",
      "Epoch 119 | MLP (loss/acc/sat): 0.459/0.805/0.541(0.546) | KAN (loss/acc/sat): 0.447/0.821/0.553(0.560)\n",
      "Epoch 120 | MLP (loss/acc/sat): 0.458/0.805/0.542(0.546) | KAN (loss/acc/sat): 0.446/0.821/0.554(0.560)\n",
      "Epoch 121 | MLP (loss/acc/sat): 0.458/0.805/0.542(0.547) | KAN (loss/acc/sat): 0.445/0.823/0.555(0.561)\n",
      "Epoch 122 | MLP (loss/acc/sat): 0.457/0.806/0.543(0.547) | KAN (loss/acc/sat): 0.444/0.823/0.556(0.562)\n",
      "Epoch 123 | MLP (loss/acc/sat): 0.457/0.807/0.543(0.548) | KAN (loss/acc/sat): 0.443/0.823/0.557(0.563)\n",
      "Epoch 124 | MLP (loss/acc/sat): 0.456/0.807/0.544(0.548) | KAN (loss/acc/sat): 0.442/0.823/0.558(0.564)\n",
      "Epoch 125 | MLP (loss/acc/sat): 0.456/0.807/0.544(0.549) | KAN (loss/acc/sat): 0.442/0.823/0.558(0.564)\n",
      "Epoch 126 | MLP (loss/acc/sat): 0.455/0.807/0.545(0.549) | KAN (loss/acc/sat): 0.441/0.824/0.559(0.565)\n",
      "Epoch 127 | MLP (loss/acc/sat): 0.455/0.807/0.545(0.550) | KAN (loss/acc/sat): 0.440/0.824/0.560(0.566)\n",
      "Epoch 128 | MLP (loss/acc/sat): 0.454/0.808/0.546(0.550) | KAN (loss/acc/sat): 0.439/0.824/0.561(0.567)\n",
      "Epoch 129 | MLP (loss/acc/sat): 0.454/0.809/0.546(0.550) | KAN (loss/acc/sat): 0.439/0.824/0.561(0.567)\n",
      "Epoch 130 | MLP (loss/acc/sat): 0.454/0.810/0.546(0.551) | KAN (loss/acc/sat): 0.438/0.824/0.562(0.568)\n",
      "Epoch 131 | MLP (loss/acc/sat): 0.453/0.811/0.547(0.551) | KAN (loss/acc/sat): 0.438/0.824/0.562(0.569)\n",
      "Epoch 132 | MLP (loss/acc/sat): 0.453/0.811/0.547(0.552) | KAN (loss/acc/sat): 0.437/0.824/0.563(0.569)\n",
      "Epoch 133 | MLP (loss/acc/sat): 0.452/0.811/0.548(0.552) | KAN (loss/acc/sat): 0.436/0.824/0.564(0.570)\n",
      "Epoch 134 | MLP (loss/acc/sat): 0.452/0.812/0.548(0.552) | KAN (loss/acc/sat): 0.436/0.824/0.564(0.570)\n",
      "Epoch 135 | MLP (loss/acc/sat): 0.452/0.812/0.548(0.553) | KAN (loss/acc/sat): 0.435/0.824/0.565(0.571)\n",
      "Epoch 136 | MLP (loss/acc/sat): 0.451/0.812/0.549(0.553) | KAN (loss/acc/sat): 0.435/0.825/0.565(0.571)\n",
      "Epoch 137 | MLP (loss/acc/sat): 0.451/0.812/0.549(0.553) | KAN (loss/acc/sat): 0.434/0.825/0.566(0.572)\n",
      "Epoch 138 | MLP (loss/acc/sat): 0.451/0.812/0.549(0.554) | KAN (loss/acc/sat): 0.433/0.825/0.567(0.573)\n",
      "Epoch 139 | MLP (loss/acc/sat): 0.450/0.812/0.550(0.554) | KAN (loss/acc/sat): 0.433/0.825/0.567(0.573)\n",
      "Epoch 140 | MLP (loss/acc/sat): 0.450/0.812/0.550(0.554) | KAN (loss/acc/sat): 0.432/0.826/0.568(0.574)\n",
      "Epoch 141 | MLP (loss/acc/sat): 0.450/0.812/0.550(0.555) | KAN (loss/acc/sat): 0.432/0.826/0.568(0.574)\n",
      "Epoch 142 | MLP (loss/acc/sat): 0.449/0.812/0.551(0.555) | KAN (loss/acc/sat): 0.431/0.826/0.569(0.575)\n",
      "Epoch 143 | MLP (loss/acc/sat): 0.449/0.812/0.551(0.555) | KAN (loss/acc/sat): 0.431/0.826/0.569(0.575)\n",
      "Epoch 144 | MLP (loss/acc/sat): 0.449/0.812/0.551(0.555) | KAN (loss/acc/sat): 0.430/0.826/0.570(0.576)\n",
      "Epoch 145 | MLP (loss/acc/sat): 0.448/0.812/0.552(0.556) | KAN (loss/acc/sat): 0.430/0.826/0.570(0.576)\n",
      "Epoch 146 | MLP (loss/acc/sat): 0.448/0.812/0.552(0.556) | KAN (loss/acc/sat): 0.429/0.826/0.571(0.577)\n",
      "Epoch 147 | MLP (loss/acc/sat): 0.448/0.812/0.552(0.556) | KAN (loss/acc/sat): 0.429/0.827/0.571(0.577)\n",
      "Epoch 148 | MLP (loss/acc/sat): 0.448/0.812/0.552(0.556) | KAN (loss/acc/sat): 0.428/0.827/0.572(0.578)\n",
      "Epoch 149 | MLP (loss/acc/sat): 0.447/0.813/0.553(0.557) | KAN (loss/acc/sat): 0.428/0.827/0.572(0.578)\n",
      "Epoch 150 | MLP (loss/acc/sat): 0.447/0.812/0.553(0.557) | KAN (loss/acc/sat): 0.428/0.827/0.572(0.578)\n"
     ]
    }
   ],
   "source": [
    "# Define the DataLoader adapted to the LTN input format. 'data' is same, 'labels' is numeric (not one-hot)\n",
    "train_loader = DataLoader(data=dataset['train_input'], labels=torch.tensor(train_y, dtype=torch.long, device=device), batch_size=len(dataset['train_input']))\n",
    "test_loader = DataLoader(data=dataset['test_input'], labels=torch.tensor(test_y, dtype=torch.long, device=device), batch_size=len(dataset['test_input']))\n",
    "\n",
    "# filter the data by label_L1, data is a tensor(n*20), label_L1 is a tensor(n*4)\n",
    "# based on the label_L1, we store the data into different ltn.Variable\n",
    "def compute_sat_levels(loader, P):\n",
    "\tsat_level  = 0\n",
    "\tfor data, labels in loader:\n",
    "\t\tx = ltn.Variable(\"x\", data)\n",
    "\t\tx_MQTT = ltn.Variable(\"x_MQTT\", data[labels == 0])\n",
    "\t\tx_Benign = ltn.Variable(\"x_Benign\", data[labels == 1])\n",
    "\t\tx_Recon = ltn.Variable(\"x_Recon\", data[labels == 2])\n",
    "\t\tx_ARP_Spoofing = ltn.Variable(\"x_ARP_Spoofing\", data[labels == 3])\n",
    "\t\t\n",
    "\t\tsat_level = SatAgg(\n",
    "\t\t\tForall(x_MQTT, P(x_MQTT, l_MQTT)),\n",
    "\t\t\tForall(x_Benign, P(x_Benign, l_Benign)),\n",
    "\t\t\tForall(x_Recon, P(x_Recon, l_Recon)),\n",
    "\t\t\tForall(x_ARP_Spoofing, P(x_ARP_Spoofing, l_ARP_Spoofing))\n",
    "\t\t)\n",
    "\treturn sat_level\n",
    "\n",
    "\n",
    "def compute_accuracy(loader, model):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data, labels in loader:\n",
    "        logits = model(data)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.numel()\n",
    "    return total_correct.float() / total_samples\n",
    "    \n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(P_mlp.parameters(), lr=0.0015)\n",
    "optimizer_kan = torch.optim.Adam(P_kan.parameters(), lr=0.0015)\n",
    "\n",
    "for epoch in range(151):\n",
    "\t# Train the MLP\n",
    "    optimizer_mlp.zero_grad()\n",
    "    sat_mlp = compute_sat_levels(train_loader, P_mlp)\n",
    "    loss = 1. - sat_mlp\n",
    "    loss.backward()\n",
    "    optimizer_mlp.step()\n",
    "    train_loss_mlp  = loss.item()\n",
    "\n",
    "    # Train the KAN\n",
    "    optimizer_kan.zero_grad()\n",
    "    sat_kan = compute_sat_levels(train_loader, P_kan)\n",
    "    loss = 1. - sat_kan\n",
    "    loss.backward()\n",
    "    optimizer_kan.step()\n",
    "    train_loss_kan = loss.item()\n",
    "\t\n",
    "    # Test\n",
    "    acc_mlp = compute_accuracy(test_loader, mlp)\n",
    "    acc_kan = compute_accuracy(test_loader, kan)\n",
    "\n",
    "    test_sat_mlp = compute_sat_levels(test_loader, P_mlp)\n",
    "    test_sat_kan = compute_sat_levels(test_loader, P_kan)\n",
    "\n",
    "    print(f\"Epoch {epoch} | MLP (loss/acc/sat): {train_loss_mlp:.3f}/{acc_mlp:.3f}/{sat_mlp:.3f}({test_sat_mlp:.3f}) | KAN (loss/acc/sat): {train_loss_kan:.3f}/{acc_kan:.3f}/{sat_kan:.3f}({test_sat_kan:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
