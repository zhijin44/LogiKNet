{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "**KAN: prepare dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Data prepared.\n",
      "Train set: (torch.Size([89918, 18]), torch.Size([89918, 4]))\n",
      "Test set: (torch.Size([9991, 18]), torch.Size([9991, 4]))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', \n",
    "    # 'Drate',\n",
    "    # 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number',\n",
    "    # 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "    # 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
    "    # 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', \n",
    "    # 'IPv','LLC', \n",
    "    'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
    "    'Magnitue', 'Radius', 'Covariance',\n",
    "    'Variance', 'Weight'\n",
    "]\n",
    "\n",
    "Y_columns = ['label_L1']\n",
    "\n",
    "label_L1_mapping = {\"MQTT\": 0, \"Benign\": 1, \"Recon\": 2, \"ARP_Spoofing\": 3}\n",
    "label_L2_mapping = {\"MQTT-DDoS-Connect_Flood\": 4, \"MQTT-DDoS-Publish_Flood\": 5, \n",
    "                    \"MQTT-DoS-Connect_Flood\": 6, \"MQTT-DoS-Publish_Flood\": 7,\n",
    "                    \"MQTT-Malformed_Data\": 8, \"benign\": 9, \n",
    "                    \"Recon-OS_Scan\": 10, \"Recon-Port_Scan\": 11,\n",
    "                    \"arp_spoofing\": 12}\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/home/zyang44/Github/baseline_cicIOT/CIC_IoMT/19classes/filtered_train_l_4_11.csv')\n",
    "df['label_L1'] = df['label_L1'].map(label_L1_mapping)\n",
    "df['label_L2'] = df['label_L2'].map(label_L1_mapping)\n",
    "\n",
    "# Shuffle the dataframe before splitting into training and test sets\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "# 90% as training set and 10% as test set\n",
    "train_size = int(len(df) * 0.9)\n",
    "train_df, test_df = df.iloc[:train_size, :], df.iloc[train_size:, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[X_columns])\n",
    "test_X_scaled = scaler.transform(test_df[X_columns])\n",
    "train_y = train_df[Y_columns].values.ravel()\n",
    "test_y = test_df[Y_columns].values.ravel()\n",
    "\n",
    "# take Y_columns as the label, and transfering to one-hot coded\n",
    "dataset = {\n",
    "    'train_input': torch.tensor(train_X_scaled, dtype=torch.float32, device=device),\n",
    "    'train_label': F.one_hot(torch.tensor(train_y, dtype=torch.long, device=device), num_classes=4),\n",
    "    'test_input': torch.tensor(test_X_scaled, dtype=torch.float32, device=device),\n",
    "    'test_label': F.one_hot(torch.tensor(test_y, dtype=torch.long, device=device), num_classes=4)\n",
    "}\n",
    "print(\"Data prepared.\",\n",
    "      f\"Train set: {dataset['train_input'].shape, dataset['train_label'].shape}\",\n",
    "      f\"Test set: {dataset['test_input'].shape, dataset['test_label'].shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build myKAN**\n",
    "\n",
    "A wrapper function, to only get the logits output by the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiKANModel(nn.Module):\n",
    "    def __init__(self, kan):\n",
    "        \"\"\"\n",
    "        Wrap an already built MultKAN instance.\n",
    "        Args:\n",
    "            kan: a MultKAN model (which has attributes such as act_fun, symbolic_fun, node_bias, node_scale,\n",
    "                 subnode_bias, subnode_scale, depth, width, mult_homo, mult_arity, input_id, symbolic_enabled, etc.)\n",
    "        \"\"\"\n",
    "        super(MultiKANModel, self).__init__()\n",
    "        self.kan = kan\n",
    "\n",
    "    def forward(self, x, training=False, singularity_avoiding=False, y_th=10.):\n",
    "        # Select input features according to input_id\n",
    "        x = x[:, self.kan.input_id.long()]\n",
    "        # Loop through each layer\n",
    "        for l in range(self.kan.depth):\n",
    "            # Get outputs from the numerical branch (KANLayer) of current layer\n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.kan.act_fun[l](x)\n",
    "            # Get output from the symbolic branch if enabled\n",
    "            if self.kan.symbolic_enabled:\n",
    "                x_symbolic, postacts_symbolic = self.kan.symbolic_fun[l](x, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            else:\n",
    "                x_symbolic = 0.\n",
    "            # Sum the numerical and symbolic outputs\n",
    "            x = x_numerical + x_symbolic\n",
    "\n",
    "            # Subnode affine transformation\n",
    "            x = self.kan.subnode_scale[l][None, :] * x + self.kan.subnode_bias[l][None, :]\n",
    "\n",
    "            # Process multiplication nodes\n",
    "            dim_sum = self.kan.width[l+1][0]\n",
    "            dim_mult = self.kan.width[l+1][1]\n",
    "            if dim_mult > 0:\n",
    "                if self.kan.mult_homo:\n",
    "                    for i in range(self.kan.mult_arity-1):\n",
    "                        if i == 0:\n",
    "                            x_mult = x[:, dim_sum::self.kan.mult_arity] * x[:, dim_sum+1::self.kan.mult_arity]\n",
    "                        else:\n",
    "                            x_mult = x_mult * x[:, dim_sum+i+1::self.kan.mult_arity]\n",
    "                else:\n",
    "                    for j in range(dim_mult):\n",
    "                        acml_id = dim_sum + int(np.sum(self.kan.mult_arity[l+1][:j]))\n",
    "                        for i in range(self.kan.mult_arity[l+1][j]-1):\n",
    "                            if i == 0:\n",
    "                                x_mult_j = x[:, [acml_id]] * x[:, [acml_id+1]]\n",
    "                            else:\n",
    "                                x_mult_j = x_mult_j * x[:, [acml_id+i+1]]\n",
    "                        if j == 0:\n",
    "                            x_mult = x_mult_j\n",
    "                        else:\n",
    "                            x_mult = torch.cat([x_mult, x_mult_j], dim=1)\n",
    "                # Concatenate sum and mult parts\n",
    "                x = torch.cat([x[:, :dim_sum], x_mult], dim=1)\n",
    "\n",
    "            # Node affine transformation\n",
    "            x = self.kan.node_scale[l][None, :] * x + self.kan.node_bias[l][None, :]\n",
    "\n",
    "        # Final x corresponds to the logits output of the whole model\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LTN Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import ltn.fuzzy_ops\n",
    "\n",
    "# define the connectives, quantifiers, and the SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())   # And = ltn.Connective(custom_fuzzy_ops.AndProd())\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "# define ltn constants\n",
    "l_MQTT = ltn.Constant(torch.tensor([1, 0, 0, 0]))\n",
    "l_Benign = ltn.Constant(torch.tensor([0, 1, 0, 0]))\n",
    "l_Recon = ltn.Constant(torch.tensor([0, 0, 1, 0]))\n",
    "l_ARP_Spoofing = ltn.Constant(torch.tensor([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "from utils import MLP, LogitsToPredicate, DataLoader, DataLoaderMulti\n",
    "from kan import KAN\n",
    "\n",
    "# Define the MLP predicate\n",
    "mlp = MLP(layer_sizes=(18, 10, 4)).to(device)\n",
    "P_mlp = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# Define myKAN predicate\n",
    "kan = KAN(width=[18, 10, 4], grid=5, k=3, seed=42, device=device)\n",
    "mykan = MultiKANModel(kan)\n",
    "P_kan = ltn.Predicate(LogitsToPredicate(mykan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN effect**\n",
    "\n",
    "LTN(kan) v.s LTN(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | MLP (loss/acc/sat): 0.731/0.386/0.269(0.271) | KAN (loss/acc/sat): 0.750/0.204/0.250(0.252)\n",
      "Epoch 1 | MLP (loss/acc/sat): 0.729/0.410/0.271(0.273) | KAN (loss/acc/sat): 0.748/0.206/0.252(0.253)\n",
      "Epoch 2 | MLP (loss/acc/sat): 0.727/0.432/0.273(0.275) | KAN (loss/acc/sat): 0.747/0.207/0.253(0.255)\n",
      "Epoch 3 | MLP (loss/acc/sat): 0.725/0.429/0.275(0.277) | KAN (loss/acc/sat): 0.745/0.206/0.255(0.256)\n",
      "Epoch 4 | MLP (loss/acc/sat): 0.723/0.429/0.277(0.279) | KAN (loss/acc/sat): 0.744/0.205/0.256(0.257)\n",
      "Epoch 5 | MLP (loss/acc/sat): 0.721/0.424/0.279(0.281) | KAN (loss/acc/sat): 0.743/0.205/0.257(0.259)\n",
      "Epoch 6 | MLP (loss/acc/sat): 0.719/0.452/0.281(0.283) | KAN (loss/acc/sat): 0.741/0.206/0.259(0.260)\n",
      "Epoch 7 | MLP (loss/acc/sat): 0.717/0.483/0.283(0.285) | KAN (loss/acc/sat): 0.740/0.209/0.260(0.262)\n",
      "Epoch 8 | MLP (loss/acc/sat): 0.715/0.479/0.285(0.287) | KAN (loss/acc/sat): 0.739/0.218/0.261(0.263)\n",
      "Epoch 9 | MLP (loss/acc/sat): 0.714/0.478/0.286(0.289) | KAN (loss/acc/sat): 0.737/0.275/0.263(0.264)\n",
      "Epoch 10 | MLP (loss/acc/sat): 0.712/0.479/0.288(0.290) | KAN (loss/acc/sat): 0.736/0.283/0.264(0.266)\n",
      "Epoch 11 | MLP (loss/acc/sat): 0.710/0.479/0.290(0.292) | KAN (loss/acc/sat): 0.734/0.294/0.266(0.267)\n",
      "Epoch 12 | MLP (loss/acc/sat): 0.708/0.480/0.292(0.294) | KAN (loss/acc/sat): 0.733/0.301/0.267(0.269)\n",
      "Epoch 13 | MLP (loss/acc/sat): 0.706/0.481/0.294(0.296) | KAN (loss/acc/sat): 0.731/0.310/0.269(0.271)\n",
      "Epoch 14 | MLP (loss/acc/sat): 0.704/0.487/0.296(0.298) | KAN (loss/acc/sat): 0.730/0.319/0.270(0.272)\n",
      "Epoch 15 | MLP (loss/acc/sat): 0.703/0.496/0.297(0.299) | KAN (loss/acc/sat): 0.728/0.325/0.272(0.274)\n",
      "Epoch 16 | MLP (loss/acc/sat): 0.701/0.520/0.299(0.301) | KAN (loss/acc/sat): 0.726/0.329/0.274(0.275)\n",
      "Epoch 17 | MLP (loss/acc/sat): 0.699/0.555/0.301(0.303) | KAN (loss/acc/sat): 0.725/0.333/0.275(0.277)\n",
      "Epoch 18 | MLP (loss/acc/sat): 0.697/0.556/0.303(0.305) | KAN (loss/acc/sat): 0.723/0.335/0.277(0.279)\n",
      "Epoch 19 | MLP (loss/acc/sat): 0.696/0.558/0.304(0.306) | KAN (loss/acc/sat): 0.721/0.338/0.279(0.280)\n",
      "Epoch 20 | MLP (loss/acc/sat): 0.694/0.558/0.306(0.308) | KAN (loss/acc/sat): 0.720/0.340/0.280(0.282)\n",
      "Epoch 21 | MLP (loss/acc/sat): 0.692/0.557/0.308(0.310) | KAN (loss/acc/sat): 0.718/0.341/0.282(0.284)\n",
      "Epoch 22 | MLP (loss/acc/sat): 0.690/0.555/0.310(0.311) | KAN (loss/acc/sat): 0.716/0.342/0.284(0.286)\n",
      "Epoch 23 | MLP (loss/acc/sat): 0.689/0.552/0.311(0.313) | KAN (loss/acc/sat): 0.715/0.343/0.285(0.287)\n",
      "Epoch 24 | MLP (loss/acc/sat): 0.687/0.548/0.313(0.315) | KAN (loss/acc/sat): 0.713/0.344/0.287(0.289)\n",
      "Epoch 25 | MLP (loss/acc/sat): 0.686/0.544/0.314(0.316) | KAN (loss/acc/sat): 0.711/0.348/0.289(0.291)\n",
      "Epoch 26 | MLP (loss/acc/sat): 0.684/0.540/0.316(0.318) | KAN (loss/acc/sat): 0.709/0.366/0.291(0.293)\n",
      "Epoch 27 | MLP (loss/acc/sat): 0.682/0.529/0.318(0.320) | KAN (loss/acc/sat): 0.707/0.390/0.293(0.295)\n",
      "Epoch 28 | MLP (loss/acc/sat): 0.681/0.514/0.319(0.321) | KAN (loss/acc/sat): 0.706/0.472/0.294(0.296)\n",
      "Epoch 29 | MLP (loss/acc/sat): 0.679/0.491/0.321(0.323) | KAN (loss/acc/sat): 0.704/0.475/0.296(0.298)\n",
      "Epoch 30 | MLP (loss/acc/sat): 0.678/0.457/0.322(0.324) | KAN (loss/acc/sat): 0.702/0.480/0.298(0.300)\n",
      "Epoch 31 | MLP (loss/acc/sat): 0.676/0.416/0.324(0.326) | KAN (loss/acc/sat): 0.700/0.483/0.300(0.302)\n",
      "Epoch 32 | MLP (loss/acc/sat): 0.674/0.388/0.326(0.328) | KAN (loss/acc/sat): 0.698/0.487/0.302(0.304)\n",
      "Epoch 33 | MLP (loss/acc/sat): 0.673/0.382/0.327(0.329) | KAN (loss/acc/sat): 0.697/0.489/0.303(0.306)\n",
      "Epoch 34 | MLP (loss/acc/sat): 0.671/0.381/0.329(0.331) | KAN (loss/acc/sat): 0.695/0.491/0.305(0.308)\n",
      "Epoch 35 | MLP (loss/acc/sat): 0.670/0.381/0.330(0.332) | KAN (loss/acc/sat): 0.693/0.494/0.307(0.309)\n",
      "Epoch 36 | MLP (loss/acc/sat): 0.668/0.382/0.332(0.333) | KAN (loss/acc/sat): 0.691/0.497/0.309(0.311)\n",
      "Epoch 37 | MLP (loss/acc/sat): 0.667/0.382/0.333(0.335) | KAN (loss/acc/sat): 0.689/0.500/0.311(0.313)\n",
      "Epoch 38 | MLP (loss/acc/sat): 0.666/0.383/0.334(0.336) | KAN (loss/acc/sat): 0.687/0.503/0.313(0.315)\n",
      "Epoch 39 | MLP (loss/acc/sat): 0.664/0.384/0.336(0.338) | KAN (loss/acc/sat): 0.686/0.506/0.314(0.317)\n",
      "Epoch 40 | MLP (loss/acc/sat): 0.663/0.386/0.337(0.339) | KAN (loss/acc/sat): 0.684/0.510/0.316(0.319)\n",
      "Epoch 41 | MLP (loss/acc/sat): 0.661/0.387/0.339(0.340) | KAN (loss/acc/sat): 0.682/0.512/0.318(0.321)\n",
      "Epoch 42 | MLP (loss/acc/sat): 0.660/0.387/0.340(0.342) | KAN (loss/acc/sat): 0.680/0.515/0.320(0.323)\n",
      "Epoch 43 | MLP (loss/acc/sat): 0.659/0.389/0.341(0.343) | KAN (loss/acc/sat): 0.678/0.519/0.322(0.325)\n",
      "Epoch 44 | MLP (loss/acc/sat): 0.657/0.391/0.343(0.344) | KAN (loss/acc/sat): 0.676/0.522/0.324(0.327)\n",
      "Epoch 45 | MLP (loss/acc/sat): 0.656/0.392/0.344(0.346) | KAN (loss/acc/sat): 0.674/0.525/0.326(0.329)\n",
      "Epoch 46 | MLP (loss/acc/sat): 0.655/0.393/0.345(0.347) | KAN (loss/acc/sat): 0.672/0.525/0.328(0.332)\n",
      "Epoch 47 | MLP (loss/acc/sat): 0.654/0.397/0.346(0.348) | KAN (loss/acc/sat): 0.670/0.525/0.330(0.334)\n",
      "Epoch 48 | MLP (loss/acc/sat): 0.652/0.398/0.348(0.349) | KAN (loss/acc/sat): 0.667/0.576/0.333(0.336)\n",
      "Epoch 49 | MLP (loss/acc/sat): 0.651/0.401/0.349(0.351) | KAN (loss/acc/sat): 0.665/0.682/0.335(0.338)\n",
      "Epoch 50 | MLP (loss/acc/sat): 0.650/0.403/0.350(0.352) | KAN (loss/acc/sat): 0.663/0.691/0.337(0.341)\n",
      "Epoch 51 | MLP (loss/acc/sat): 0.649/0.405/0.351(0.353) | KAN (loss/acc/sat): 0.661/0.694/0.339(0.343)\n",
      "Epoch 52 | MLP (loss/acc/sat): 0.648/0.408/0.352(0.354) | KAN (loss/acc/sat): 0.659/0.696/0.341(0.345)\n",
      "Epoch 53 | MLP (loss/acc/sat): 0.647/0.413/0.353(0.355) | KAN (loss/acc/sat): 0.656/0.700/0.344(0.348)\n",
      "Epoch 54 | MLP (loss/acc/sat): 0.645/0.417/0.355(0.357) | KAN (loss/acc/sat): 0.654/0.701/0.346(0.350)\n",
      "Epoch 55 | MLP (loss/acc/sat): 0.644/0.420/0.356(0.358) | KAN (loss/acc/sat): 0.651/0.704/0.349(0.353)\n",
      "Epoch 56 | MLP (loss/acc/sat): 0.643/0.425/0.357(0.359) | KAN (loss/acc/sat): 0.649/0.708/0.351(0.355)\n",
      "Epoch 57 | MLP (loss/acc/sat): 0.642/0.428/0.358(0.360) | KAN (loss/acc/sat): 0.646/0.713/0.354(0.358)\n",
      "Epoch 58 | MLP (loss/acc/sat): 0.641/0.432/0.359(0.361) | KAN (loss/acc/sat): 0.644/0.719/0.356(0.361)\n",
      "Epoch 59 | MLP (loss/acc/sat): 0.640/0.436/0.360(0.362) | KAN (loss/acc/sat): 0.641/0.726/0.359(0.363)\n",
      "Epoch 60 | MLP (loss/acc/sat): 0.639/0.438/0.361(0.363) | KAN (loss/acc/sat): 0.639/0.732/0.361(0.366)\n",
      "Epoch 61 | MLP (loss/acc/sat): 0.638/0.441/0.362(0.364) | KAN (loss/acc/sat): 0.636/0.736/0.364(0.369)\n",
      "Epoch 62 | MLP (loss/acc/sat): 0.637/0.445/0.363(0.365) | KAN (loss/acc/sat): 0.633/0.741/0.367(0.372)\n",
      "Epoch 63 | MLP (loss/acc/sat): 0.636/0.447/0.364(0.367) | KAN (loss/acc/sat): 0.630/0.748/0.370(0.375)\n",
      "Epoch 64 | MLP (loss/acc/sat): 0.635/0.450/0.365(0.368) | KAN (loss/acc/sat): 0.628/0.754/0.372(0.378)\n",
      "Epoch 65 | MLP (loss/acc/sat): 0.633/0.455/0.367(0.369) | KAN (loss/acc/sat): 0.625/0.759/0.375(0.380)\n",
      "Epoch 66 | MLP (loss/acc/sat): 0.632/0.460/0.368(0.370) | KAN (loss/acc/sat): 0.622/0.764/0.378(0.383)\n",
      "Epoch 67 | MLP (loss/acc/sat): 0.631/0.464/0.369(0.371) | KAN (loss/acc/sat): 0.619/0.768/0.381(0.386)\n",
      "Epoch 68 | MLP (loss/acc/sat): 0.630/0.468/0.370(0.372) | KAN (loss/acc/sat): 0.616/0.770/0.384(0.389)\n",
      "Epoch 69 | MLP (loss/acc/sat): 0.629/0.485/0.371(0.373) | KAN (loss/acc/sat): 0.613/0.774/0.387(0.393)\n",
      "Epoch 70 | MLP (loss/acc/sat): 0.628/0.521/0.372(0.374) | KAN (loss/acc/sat): 0.610/0.776/0.390(0.396)\n",
      "Epoch 71 | MLP (loss/acc/sat): 0.627/0.536/0.373(0.375) | KAN (loss/acc/sat): 0.607/0.779/0.393(0.399)\n",
      "Epoch 72 | MLP (loss/acc/sat): 0.626/0.542/0.374(0.376) | KAN (loss/acc/sat): 0.604/0.781/0.396(0.402)\n",
      "Epoch 73 | MLP (loss/acc/sat): 0.625/0.543/0.375(0.377) | KAN (loss/acc/sat): 0.601/0.782/0.399(0.405)\n",
      "Epoch 74 | MLP (loss/acc/sat): 0.624/0.546/0.376(0.378) | KAN (loss/acc/sat): 0.598/0.783/0.402(0.408)\n",
      "Epoch 75 | MLP (loss/acc/sat): 0.623/0.548/0.377(0.379) | KAN (loss/acc/sat): 0.595/0.784/0.405(0.412)\n",
      "Epoch 76 | MLP (loss/acc/sat): 0.622/0.549/0.378(0.380) | KAN (loss/acc/sat): 0.591/0.785/0.409(0.415)\n",
      "Epoch 77 | MLP (loss/acc/sat): 0.621/0.550/0.379(0.381) | KAN (loss/acc/sat): 0.588/0.786/0.412(0.418)\n",
      "Epoch 78 | MLP (loss/acc/sat): 0.620/0.551/0.380(0.382) | KAN (loss/acc/sat): 0.585/0.788/0.415(0.422)\n",
      "Epoch 79 | MLP (loss/acc/sat): 0.619/0.552/0.381(0.383) | KAN (loss/acc/sat): 0.582/0.789/0.418(0.425)\n",
      "Epoch 80 | MLP (loss/acc/sat): 0.618/0.552/0.382(0.384) | KAN (loss/acc/sat): 0.578/0.790/0.422(0.428)\n",
      "Epoch 81 | MLP (loss/acc/sat): 0.618/0.555/0.382(0.385) | KAN (loss/acc/sat): 0.575/0.791/0.425(0.432)\n",
      "Epoch 82 | MLP (loss/acc/sat): 0.617/0.556/0.383(0.386) | KAN (loss/acc/sat): 0.572/0.791/0.428(0.435)\n",
      "Epoch 83 | MLP (loss/acc/sat): 0.616/0.558/0.384(0.387) | KAN (loss/acc/sat): 0.568/0.792/0.432(0.438)\n",
      "Epoch 84 | MLP (loss/acc/sat): 0.615/0.558/0.385(0.388) | KAN (loss/acc/sat): 0.565/0.793/0.435(0.442)\n",
      "Epoch 85 | MLP (loss/acc/sat): 0.614/0.558/0.386(0.389) | KAN (loss/acc/sat): 0.562/0.795/0.438(0.445)\n",
      "Epoch 86 | MLP (loss/acc/sat): 0.613/0.559/0.387(0.390) | KAN (loss/acc/sat): 0.558/0.796/0.442(0.448)\n",
      "Epoch 87 | MLP (loss/acc/sat): 0.612/0.558/0.388(0.391) | KAN (loss/acc/sat): 0.555/0.797/0.445(0.452)\n",
      "Epoch 88 | MLP (loss/acc/sat): 0.611/0.558/0.389(0.392) | KAN (loss/acc/sat): 0.552/0.797/0.448(0.455)\n",
      "Epoch 89 | MLP (loss/acc/sat): 0.610/0.558/0.390(0.393) | KAN (loss/acc/sat): 0.549/0.798/0.451(0.458)\n",
      "Epoch 90 | MLP (loss/acc/sat): 0.609/0.558/0.391(0.394) | KAN (loss/acc/sat): 0.545/0.799/0.455(0.462)\n",
      "Epoch 91 | MLP (loss/acc/sat): 0.608/0.558/0.392(0.395) | KAN (loss/acc/sat): 0.542/0.800/0.458(0.465)\n",
      "Epoch 92 | MLP (loss/acc/sat): 0.608/0.559/0.392(0.395) | KAN (loss/acc/sat): 0.539/0.801/0.461(0.468)\n",
      "Epoch 93 | MLP (loss/acc/sat): 0.607/0.560/0.393(0.396) | KAN (loss/acc/sat): 0.536/0.801/0.464(0.471)\n",
      "Epoch 94 | MLP (loss/acc/sat): 0.606/0.560/0.394(0.397) | KAN (loss/acc/sat): 0.533/0.802/0.467(0.475)\n",
      "Epoch 95 | MLP (loss/acc/sat): 0.605/0.560/0.395(0.398) | KAN (loss/acc/sat): 0.529/0.803/0.471(0.478)\n",
      "Epoch 96 | MLP (loss/acc/sat): 0.604/0.560/0.396(0.399) | KAN (loss/acc/sat): 0.526/0.805/0.474(0.481)\n",
      "Epoch 97 | MLP (loss/acc/sat): 0.603/0.560/0.397(0.400) | KAN (loss/acc/sat): 0.523/0.808/0.477(0.484)\n",
      "Epoch 98 | MLP (loss/acc/sat): 0.603/0.561/0.397(0.401) | KAN (loss/acc/sat): 0.520/0.810/0.480(0.487)\n",
      "Epoch 99 | MLP (loss/acc/sat): 0.602/0.561/0.398(0.401) | KAN (loss/acc/sat): 0.517/0.811/0.483(0.490)\n",
      "Epoch 100 | MLP (loss/acc/sat): 0.601/0.562/0.399(0.402) | KAN (loss/acc/sat): 0.514/0.812/0.486(0.493)\n",
      "Epoch 101 | MLP (loss/acc/sat): 0.600/0.563/0.400(0.403) | KAN (loss/acc/sat): 0.512/0.814/0.488(0.496)\n",
      "Epoch 102 | MLP (loss/acc/sat): 0.599/0.563/0.401(0.404) | KAN (loss/acc/sat): 0.509/0.816/0.491(0.498)\n",
      "Epoch 103 | MLP (loss/acc/sat): 0.599/0.563/0.401(0.405) | KAN (loss/acc/sat): 0.506/0.817/0.494(0.501)\n",
      "Epoch 104 | MLP (loss/acc/sat): 0.598/0.563/0.402(0.405) | KAN (loss/acc/sat): 0.503/0.817/0.497(0.504)\n",
      "Epoch 105 | MLP (loss/acc/sat): 0.597/0.563/0.403(0.406) | KAN (loss/acc/sat): 0.501/0.817/0.499(0.507)\n",
      "Epoch 106 | MLP (loss/acc/sat): 0.596/0.563/0.404(0.407) | KAN (loss/acc/sat): 0.498/0.817/0.502(0.509)\n",
      "Epoch 107 | MLP (loss/acc/sat): 0.595/0.563/0.405(0.408) | KAN (loss/acc/sat): 0.495/0.818/0.505(0.512)\n",
      "Epoch 108 | MLP (loss/acc/sat): 0.595/0.563/0.405(0.408) | KAN (loss/acc/sat): 0.493/0.818/0.507(0.514)\n",
      "Epoch 109 | MLP (loss/acc/sat): 0.594/0.564/0.406(0.409) | KAN (loss/acc/sat): 0.490/0.818/0.510(0.517)\n",
      "Epoch 110 | MLP (loss/acc/sat): 0.593/0.564/0.407(0.410) | KAN (loss/acc/sat): 0.488/0.819/0.512(0.519)\n",
      "Epoch 111 | MLP (loss/acc/sat): 0.593/0.564/0.407(0.411) | KAN (loss/acc/sat): 0.486/0.819/0.514(0.521)\n",
      "Epoch 112 | MLP (loss/acc/sat): 0.592/0.564/0.408(0.411) | KAN (loss/acc/sat): 0.483/0.819/0.517(0.523)\n",
      "Epoch 113 | MLP (loss/acc/sat): 0.591/0.564/0.409(0.412) | KAN (loss/acc/sat): 0.481/0.819/0.519(0.526)\n",
      "Epoch 114 | MLP (loss/acc/sat): 0.590/0.564/0.410(0.413) | KAN (loss/acc/sat): 0.479/0.819/0.521(0.528)\n",
      "Epoch 115 | MLP (loss/acc/sat): 0.590/0.565/0.410(0.413) | KAN (loss/acc/sat): 0.477/0.819/0.523(0.530)\n",
      "Epoch 116 | MLP (loss/acc/sat): 0.589/0.565/0.411(0.414) | KAN (loss/acc/sat): 0.475/0.819/0.525(0.532)\n",
      "Epoch 117 | MLP (loss/acc/sat): 0.588/0.565/0.412(0.415) | KAN (loss/acc/sat): 0.473/0.819/0.527(0.534)\n",
      "Epoch 118 | MLP (loss/acc/sat): 0.588/0.566/0.412(0.416) | KAN (loss/acc/sat): 0.471/0.820/0.529(0.535)\n",
      "Epoch 119 | MLP (loss/acc/sat): 0.587/0.567/0.413(0.416) | KAN (loss/acc/sat): 0.469/0.820/0.531(0.537)\n",
      "Epoch 120 | MLP (loss/acc/sat): 0.586/0.568/0.414(0.417) | KAN (loss/acc/sat): 0.468/0.820/0.532(0.539)\n",
      "Epoch 121 | MLP (loss/acc/sat): 0.586/0.570/0.414(0.418) | KAN (loss/acc/sat): 0.466/0.820/0.534(0.540)\n",
      "Epoch 122 | MLP (loss/acc/sat): 0.585/0.571/0.415(0.418) | KAN (loss/acc/sat): 0.464/0.821/0.536(0.542)\n",
      "Epoch 123 | MLP (loss/acc/sat): 0.584/0.572/0.416(0.419) | KAN (loss/acc/sat): 0.463/0.821/0.537(0.544)\n",
      "Epoch 124 | MLP (loss/acc/sat): 0.584/0.572/0.416(0.420) | KAN (loss/acc/sat): 0.461/0.821/0.539(0.545)\n",
      "Epoch 125 | MLP (loss/acc/sat): 0.583/0.573/0.417(0.420) | KAN (loss/acc/sat): 0.460/0.821/0.540(0.546)\n",
      "Epoch 126 | MLP (loss/acc/sat): 0.582/0.573/0.418(0.421) | KAN (loss/acc/sat): 0.459/0.822/0.541(0.548)\n",
      "Epoch 127 | MLP (loss/acc/sat): 0.582/0.573/0.418(0.422) | KAN (loss/acc/sat): 0.457/0.822/0.543(0.549)\n",
      "Epoch 128 | MLP (loss/acc/sat): 0.581/0.573/0.419(0.422) | KAN (loss/acc/sat): 0.456/0.822/0.544(0.550)\n",
      "Epoch 129 | MLP (loss/acc/sat): 0.580/0.575/0.420(0.423) | KAN (loss/acc/sat): 0.455/0.822/0.545(0.551)\n",
      "Epoch 130 | MLP (loss/acc/sat): 0.580/0.576/0.420(0.424) | KAN (loss/acc/sat): 0.454/0.823/0.546(0.553)\n",
      "Epoch 131 | MLP (loss/acc/sat): 0.579/0.576/0.421(0.424) | KAN (loss/acc/sat): 0.452/0.823/0.548(0.554)\n",
      "Epoch 132 | MLP (loss/acc/sat): 0.578/0.576/0.422(0.425) | KAN (loss/acc/sat): 0.451/0.823/0.549(0.555)\n",
      "Epoch 133 | MLP (loss/acc/sat): 0.578/0.577/0.422(0.425) | KAN (loss/acc/sat): 0.450/0.822/0.550(0.556)\n",
      "Epoch 134 | MLP (loss/acc/sat): 0.577/0.577/0.423(0.426) | KAN (loss/acc/sat): 0.449/0.823/0.551(0.557)\n",
      "Epoch 135 | MLP (loss/acc/sat): 0.576/0.578/0.424(0.427) | KAN (loss/acc/sat): 0.448/0.823/0.552(0.558)\n",
      "Epoch 136 | MLP (loss/acc/sat): 0.576/0.578/0.424(0.427) | KAN (loss/acc/sat): 0.447/0.823/0.553(0.559)\n",
      "Epoch 137 | MLP (loss/acc/sat): 0.575/0.579/0.425(0.428) | KAN (loss/acc/sat): 0.446/0.823/0.554(0.559)\n",
      "Epoch 138 | MLP (loss/acc/sat): 0.575/0.581/0.425(0.429) | KAN (loss/acc/sat): 0.446/0.823/0.554(0.560)\n",
      "Epoch 139 | MLP (loss/acc/sat): 0.574/0.582/0.426(0.429) | KAN (loss/acc/sat): 0.445/0.823/0.555(0.561)\n",
      "Epoch 140 | MLP (loss/acc/sat): 0.573/0.583/0.427(0.430) | KAN (loss/acc/sat): 0.444/0.823/0.556(0.562)\n",
      "Epoch 141 | MLP (loss/acc/sat): 0.573/0.584/0.427(0.431) | KAN (loss/acc/sat): 0.443/0.824/0.557(0.563)\n",
      "Epoch 142 | MLP (loss/acc/sat): 0.572/0.584/0.428(0.431) | KAN (loss/acc/sat): 0.442/0.824/0.558(0.563)\n",
      "Epoch 143 | MLP (loss/acc/sat): 0.571/0.585/0.429(0.432) | KAN (loss/acc/sat): 0.442/0.824/0.558(0.564)\n",
      "Epoch 144 | MLP (loss/acc/sat): 0.571/0.586/0.429(0.433) | KAN (loss/acc/sat): 0.441/0.824/0.559(0.565)\n",
      "Epoch 145 | MLP (loss/acc/sat): 0.570/0.586/0.430(0.433) | KAN (loss/acc/sat): 0.440/0.824/0.560(0.565)\n",
      "Epoch 146 | MLP (loss/acc/sat): 0.569/0.586/0.431(0.434) | KAN (loss/acc/sat): 0.440/0.825/0.560(0.566)\n",
      "Epoch 147 | MLP (loss/acc/sat): 0.569/0.587/0.431(0.434) | KAN (loss/acc/sat): 0.439/0.825/0.561(0.567)\n",
      "Epoch 148 | MLP (loss/acc/sat): 0.568/0.588/0.432(0.435) | KAN (loss/acc/sat): 0.438/0.825/0.562(0.567)\n",
      "Epoch 149 | MLP (loss/acc/sat): 0.567/0.588/0.433(0.436) | KAN (loss/acc/sat): 0.438/0.825/0.562(0.568)\n",
      "Epoch 150 | MLP (loss/acc/sat): 0.567/0.589/0.433(0.436) | KAN (loss/acc/sat): 0.437/0.825/0.563(0.568)\n"
     ]
    }
   ],
   "source": [
    "# Define the DataLoader adapted to the LTN input format. 'data' is same, 'labels' is numeric (not one-hot)\n",
    "train_loader = DataLoader(data=dataset['train_input'], labels=torch.tensor(train_y, dtype=torch.long, device=device), batch_size=len(dataset['train_input']))\n",
    "test_loader = DataLoader(data=dataset['test_input'], labels=torch.tensor(test_y, dtype=torch.long, device=device), batch_size=len(dataset['test_input']))\n",
    "\n",
    "# filter the data by label_L1, data is a tensor(n*20), label_L1 is a tensor(n*4)\n",
    "# based on the label_L1, we store the data into different ltn.Variable\n",
    "def compute_sat_levels(loader, P):\n",
    "\tsat_level  = 0\n",
    "\tfor data, labels in loader:\n",
    "\t\tx = ltn.Variable(\"x\", data)\n",
    "\t\tx_MQTT = ltn.Variable(\"x_MQTT\", data[labels == 0])\n",
    "\t\tx_Benign = ltn.Variable(\"x_Benign\", data[labels == 1])\n",
    "\t\tx_Recon = ltn.Variable(\"x_Recon\", data[labels == 2])\n",
    "\t\tx_ARP_Spoofing = ltn.Variable(\"x_ARP_Spoofing\", data[labels == 3])\n",
    "\t\t\n",
    "\t\tsat_level = SatAgg(\n",
    "\t\t\tForall(x_MQTT, P(x_MQTT, l_MQTT)),\n",
    "\t\t\tForall(x_Benign, P(x_Benign, l_Benign)),\n",
    "\t\t\tForall(x_Recon, P(x_Recon, l_Recon)),\n",
    "\t\t\tForall(x_ARP_Spoofing, P(x_ARP_Spoofing, l_ARP_Spoofing))\n",
    "\t\t)\n",
    "\treturn sat_level\n",
    "\n",
    "\n",
    "def compute_accuracy(loader, model):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data, labels in loader:\n",
    "        logits = model(data)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.numel()\n",
    "    return total_correct.float() / total_samples\n",
    "    \n",
    "\n",
    "optimizer_mlp = torch.optim.Adam(P_mlp.parameters(), lr=0.0015)\n",
    "optimizer_kan = torch.optim.Adam(P_kan.parameters(), lr=0.0015)\n",
    "\n",
    "for epoch in range(151):\n",
    "\t# Train the MLP\n",
    "    optimizer_mlp.zero_grad()\n",
    "    sat_mlp = compute_sat_levels(train_loader, P_mlp)\n",
    "    loss = 1. - sat_mlp\n",
    "    loss.backward()\n",
    "    optimizer_mlp.step()\n",
    "    train_loss_mlp  = loss.item()\n",
    "\n",
    "    # Train the KAN\n",
    "    optimizer_kan.zero_grad()\n",
    "    sat_kan = compute_sat_levels(train_loader, P_kan)\n",
    "    loss = 1. - sat_kan\n",
    "    loss.backward()\n",
    "    optimizer_kan.step()\n",
    "    train_loss_kan = loss.item()\n",
    "\t\n",
    "    # Test\n",
    "    acc_mlp = compute_accuracy(test_loader, mlp)\n",
    "    acc_kan = compute_accuracy(test_loader, kan)\n",
    "\n",
    "    test_sat_mlp = compute_sat_levels(test_loader, P_mlp)\n",
    "    test_sat_kan = compute_sat_levels(test_loader, P_kan)\n",
    "\n",
    "    print(f\"Epoch {epoch} | MLP (loss/acc/sat): {train_loss_mlp:.3f}/{acc_mlp:.3f}/{sat_mlp:.3f}({test_sat_mlp:.3f}) | KAN (loss/acc/sat): {train_loss_kan:.3f}/{acc_kan:.3f}/{sat_kan:.3f}({test_sat_kan:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key observation:\n",
    "* [Convergence] loss converge faster. \n",
    "* [SAT] the overall Sat level higher, means that it converges along with the rules satisfied well.\n",
    "* [Acc] accuracy converge eariler, and slightly better at the end.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
