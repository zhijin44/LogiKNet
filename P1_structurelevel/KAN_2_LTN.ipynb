{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Data prepared.\n",
      "Train set: (torch.Size([33048, 20]), torch.Size([33048, 4]))\n",
      "Test set: (torch.Size([3672, 20]), torch.Size([3672, 4]))\n"
     ]
    }
   ],
   "source": [
    "from kan import KAN\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', \n",
    "    # 'Drate',\n",
    "    # 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number',\n",
    "    # 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "    # 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
    "    # 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', \n",
    "    'IPv','LLC', \n",
    "    'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
    "    'Magnitue', 'Radius', 'Covariance',\n",
    "    'Variance', 'Weight'\n",
    "]\n",
    "\n",
    "Y_columns = ['label_L1']\n",
    "\n",
    "label_L1_mapping = {\"MQTT\": 0, \"Benign\": 1, \"Recon\": 2, \"ARP_Spoofing\": 3}\n",
    "label_L2_mapping = {\"MQTT-DDoS-Connect_Flood\": 4, \"MQTT-DDoS-Publish_Flood\": 5, \n",
    "                    \"MQTT-DoS-Connect_Flood\": 6, \"MQTT-DoS-Publish_Flood\": 7,\n",
    "                    \"MQTT-Malformed_Data\": 8, \"benign\": 9, \n",
    "                    \"Recon-OS_Scan\": 10, \"Recon-Port_Scan\": 11,\n",
    "                    \"arp_spoofing\": 12}\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/home/zyang44/Github/baseline_cicIOT/CIC_IoMT/19classes/filtered_train_s_4_11.csv')\n",
    "\n",
    "df['label_L1'] = df['label_L1'].map(label_L1_mapping)\n",
    "df['label_L2'] = df['label_L2'].map(label_L1_mapping)\n",
    "\n",
    "# 90% as training set and 10% left as test set\n",
    "train_size = int(len(df) * 0.9)\n",
    "train_df, test_df = df.iloc[:train_size, :], df.iloc[train_size:, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[X_columns])\n",
    "test_X_scaled = scaler.transform(test_df[X_columns])\n",
    "train_y = train_df[Y_columns].values.ravel()\n",
    "test_y = test_df[Y_columns].values.ravel()\n",
    "\n",
    "# take Y_columns as the label, and transfering to one-hot coded\n",
    "dataset = {\n",
    "    'train_input': torch.tensor(train_X_scaled, dtype=torch.float32, device=device),\n",
    "    'train_label': F.one_hot(torch.tensor(train_y, dtype=torch.long, device=device), num_classes=4),\n",
    "    'test_input': torch.tensor(test_X_scaled, dtype=torch.float32, device=device),\n",
    "    'test_label': F.one_hot(torch.tensor(test_y, dtype=torch.long, device=device), num_classes=4)\n",
    "}\n",
    "print(\"Data prepared.\",\n",
    "      f\"Train set: {dataset['train_input'].shape, dataset['train_label'].shape}\",\n",
    "      f\"Test set: {dataset['test_input'].shape, dataset['test_label'].shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.66e-01 | test_loss: 5.64e-01 | reg: 1.20e+03 | : 100%|â–ˆ| 100/100 [00:04<00:00, 23.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n",
      "KAN output:\n",
      "Train logits torch.Size([33048, 4])\n",
      "Test logits torch.Size([3672, 4])\n"
     ]
    }
   ],
   "source": [
    "# create a KAN\n",
    "kan = KAN(width=[20, 10, 4], grid=5, k=3, seed=42, device=device)\n",
    "\n",
    "results = kan.fit(dataset, opt=\"Adam\", steps=100)\n",
    "\n",
    "train_logits = kan(dataset['train_input'])\n",
    "test_logits = kan(dataset['test_input'])\n",
    "print(\"KAN output:\",\n",
    "      f\"Train logits {train_logits.shape}\",\n",
    "      f\"Test logits {test_logits.shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import ltn.fuzzy_ops\n",
    "from utils import MLP, LogitsToPredicate, DataLoader, DataLoaderMulti\n",
    "\n",
    "# define the connectives, quantifiers, and the SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())   # And = ltn.Connective(custom_fuzzy_ops.AndProd())\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "# define ltn constants\n",
    "l_MQTT = ltn.Constant(torch.tensor([1, 0, 0, 0]))\n",
    "l_Benign = ltn.Constant(torch.tensor([0, 1, 0, 0]))\n",
    "l_Recon = ltn.Constant(torch.tensor([0, 0, 1, 0]))\n",
    "l_ARP_Spoofing = ltn.Constant(torch.tensor([0, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(layer_sizes=(20, 10, 4)).to(device)\n",
    "P_mlp = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "train_loader = DataLoader(data=dataset['train_input'], labels=torch.tensor(train_y, dtype=torch.long, device=device), batch_size=len(dataset['train_input']))\n",
    "test_loader = DataLoader(data=dataset['test_input'], labels=torch.tensor(test_y, dtype=torch.long, device=device), batch_size=len(dataset['test_input']))\n",
    "\n",
    "# here, we need to filter the data by label_L1, data is a tensor(n*20), label_L1 is a tensor(n*4)\n",
    "# based on the label_L1, we store the data into different ltn.Variable\n",
    "# then, we can use the ltn.Predicate to get the predicate of the data\n",
    "for data, labels in train_loader:\n",
    "\tx = ltn.Variable(\"x\", train_loader.data)\n",
    "\tx_MQTT = ltn.Variable(\"x_MQTT\", data[labels == 0]) \n",
    "\tx_Benign = ltn.Variable(\"x_Benign\", data[labels == 1])\n",
    "\tx_Recon = ltn.Variable(\"x_Recon\", data[labels == 2])\n",
    "\tx_ARP_Spoofing = ltn.Variable(\"x_ARP_Spoofing\", data[labels == 3])\n",
    "\t\n",
    "\tForall(x_MQTT, P_mlp(x_MQTT, l_MQTT))\n",
    "\tForall(x_Benign, P_mlp(x_Benign, l_Benign))\n",
    "\tForall(x_Recon, P_mlp(x_Recon, l_Recon))\n",
    "\tForall(x_ARP_Spoofing, P_mlp(x_ARP_Spoofing, l_ARP_Spoofing))\n",
    "\n",
    "\t# p_kan = P_kan(x)\n",
    "\t# p_kan_mqtt = P_kan(x_MQTT, l_MQTT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build myKAN** \n",
    "\n",
    "By using the KANLayer, below have shown that the output logits from myKAN and KAN is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiKANModel(nn.Module):\n",
    "    def __init__(self, kan):\n",
    "        \"\"\"\n",
    "        Wrap an already built MultKAN instance.\n",
    "        Args:\n",
    "            kan: a MultKAN model (which has attributes such as act_fun, symbolic_fun, node_bias, node_scale,\n",
    "                 subnode_bias, subnode_scale, depth, width, mult_homo, mult_arity, input_id, symbolic_enabled, etc.)\n",
    "        \"\"\"\n",
    "        super(MultiKANModel, self).__init__()\n",
    "        self.kan = kan\n",
    "\n",
    "    def forward(self, x, training=False, singularity_avoiding=False, y_th=10.):\n",
    "        # Select input features according to input_id\n",
    "        x = x[:, self.kan.input_id.long()]\n",
    "        # Loop through each layer\n",
    "        for l in range(self.kan.depth):\n",
    "            # Get outputs from the numerical branch (KANLayer) of current layer\n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.kan.act_fun[l](x)\n",
    "            # Get output from the symbolic branch if enabled\n",
    "            if self.kan.symbolic_enabled:\n",
    "                x_symbolic, postacts_symbolic = self.kan.symbolic_fun[l](x, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            else:\n",
    "                x_symbolic = 0.\n",
    "            # Sum the numerical and symbolic outputs\n",
    "            x = x_numerical + x_symbolic\n",
    "\n",
    "            # Subnode affine transformation\n",
    "            x = self.kan.subnode_scale[l][None, :] * x + self.kan.subnode_bias[l][None, :]\n",
    "\n",
    "            # Process multiplication nodes\n",
    "            dim_sum = self.kan.width[l+1][0]\n",
    "            dim_mult = self.kan.width[l+1][1]\n",
    "            if dim_mult > 0:\n",
    "                if self.kan.mult_homo:\n",
    "                    for i in range(self.kan.mult_arity-1):\n",
    "                        if i == 0:\n",
    "                            x_mult = x[:, dim_sum::self.kan.mult_arity] * x[:, dim_sum+1::self.kan.mult_arity]\n",
    "                        else:\n",
    "                            x_mult = x_mult * x[:, dim_sum+i+1::self.kan.mult_arity]\n",
    "                else:\n",
    "                    for j in range(dim_mult):\n",
    "                        acml_id = dim_sum + int(np.sum(self.kan.mult_arity[l+1][:j]))\n",
    "                        for i in range(self.kan.mult_arity[l+1][j]-1):\n",
    "                            if i == 0:\n",
    "                                x_mult_j = x[:, [acml_id]] * x[:, [acml_id+1]]\n",
    "                            else:\n",
    "                                x_mult_j = x_mult_j * x[:, [acml_id+i+1]]\n",
    "                        if j == 0:\n",
    "                            x_mult = x_mult_j\n",
    "                        else:\n",
    "                            x_mult = torch.cat([x_mult, x_mult_j], dim=1)\n",
    "                # Concatenate sum and mult parts\n",
    "                x = torch.cat([x[:, :dim_sum], x_mult], dim=1)\n",
    "\n",
    "            # Node affine transformation\n",
    "            x = self.kan.node_scale[l][None, :] * x + self.kan.node_bias[l][None, :]\n",
    "\n",
    "        # Final x corresponds to the logits output of the whole model\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "KAN output: tensor([-0.0136, -0.2378,  0.2716, -0.1260], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "MultiKANModel output: tensor([-0.0136, -0.2378,  0.2716, -0.1260], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "MLP output: tensor([-0.7183, -0.2437,  0.1583,  0.3623], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "kan = KAN(width=[20, 10, 4], grid=5, k=3, seed=42, device=device)\n",
    "# results = kan.fit(dataset, opt=\"Adam\", steps=100)\n",
    "\n",
    "train_logits = kan(dataset['train_input'])\n",
    "print(\"KAN output:\", train_logits[10])\n",
    "\n",
    "mykan = MultiKANModel(kan)\n",
    "mykan_logits = mykan(dataset['train_input'])\n",
    "print(\"MultiKANModel output:\", mykan_logits[10])\n",
    "\n",
    "mlp_logits = mlp(dataset['train_input'])\n",
    "print(\"MLP output:\", mlp_logits[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_mlp(x_MQTT, l_MQTT): LTNObject(value=tensor([0.1994, 0.1340, 0.2202,  ..., 0.1750, 0.2290, 0.2164], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), free_vars=['x_MQTT'])\n",
      "P_kan(x_MQTT, l_MQTT): LTNObject(value=tensor([0.2669, 0.2453, 0.2565,  ..., 0.2666, 0.2495, 0.2514], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>), free_vars=['x_MQTT'])\n"
     ]
    }
   ],
   "source": [
    "P_mlp = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "P_kan = ltn.Predicate(LogitsToPredicate(mykan))\n",
    "\n",
    "P_mlp(x_MQTT, l_MQTT)\n",
    "P_kan(x_MQTT, l_MQTT)\n",
    "print(\"P_mlp(x_MQTT, l_MQTT):\", P_mlp(x_MQTT, l_MQTT))\n",
    "print(\"P_kan(x_MQTT, l_MQTT):\", P_kan(x_MQTT, l_MQTT))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
