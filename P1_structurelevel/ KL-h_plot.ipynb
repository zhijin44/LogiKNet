{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Any NaN in test_X_scaled: False\n",
      "Any Inf in test_X_scaled: False\n",
      "Unique train_y values: [0 1 2 3 4 5]\n",
      "Unique test_y values: [0 1 2 3 4 5]\n",
      "Data prepared.\n",
      "Train set: (torch.Size([35945, 18]), torch.Size([35945, 6]))\n",
      "Test set: (torch.Size([3994, 18]), torch.Size([3994, 6]))\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils import LogitsToPredicate, MLP, MultiKANModel, DataLoader, DataLoaderMulti\n",
    "from kan import KAN\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "X_columns = [\n",
    "    'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', \n",
    "    # 'Drate',\n",
    "    # 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number',\n",
    "    # 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
    "    # 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet',\n",
    "    # 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', \n",
    "    'IPv','LLC', \n",
    "    'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number',\n",
    "    'Magnitue', 'Radius', 'Covariance',\n",
    "    # 'Variance', 'Weight'\n",
    "]\n",
    "\n",
    "Y_columns = ['label_L2']\n",
    "\n",
    "label_L1_mapping = {\"MQTT\": 0, \"Benign\": 1} \n",
    "label_L2_mapping = {\"MQTT-DDoS-Connect_Flood\": 0, \"MQTT-DDoS-Publish_Flood\": 1, \n",
    "                    \"MQTT-DoS-Connect_Flood\": 2, \"MQTT-DoS-Publish_Flood\": 3,\n",
    "                    \"MQTT-Malformed_Data\": 4, \"Benign\": 5} \n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('/home/zyang44/Github/baseline_cicIOT/CIC_IoMT/19classes/filtered_train_l_2_6.csv')\n",
    "df['label_L1'] = df['label_L1'].map(label_L1_mapping)\n",
    "df['label_L2'] = df['label_L2'].map(label_L2_mapping)\n",
    "\n",
    "# Shuffle the dataframe before splitting into training and test sets\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "# 90% as training set and 10% as test set\n",
    "train_size = int(len(df) * 0.9)\n",
    "train_df, test_df = df.iloc[:train_size, :], df.iloc[train_size:, :]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[X_columns])\n",
    "test_X_scaled = scaler.transform(test_df[X_columns])\n",
    "print(\"Any NaN in test_X_scaled:\", np.isnan(test_X_scaled).any())\n",
    "print(\"Any Inf in test_X_scaled:\", np.isinf(test_X_scaled).any())\n",
    "\n",
    "train_y = train_df[Y_columns].values.ravel()\n",
    "test_y = test_df[Y_columns].values.ravel()\n",
    "print(\"Unique train_y values:\", np.unique(train_y))\n",
    "print(\"Unique test_y values:\", np.unique(test_y))\n",
    "# take Y_columns as the label, and transfering to one-hot coded\n",
    "dataset = {\n",
    "    'train_input': torch.tensor(train_X_scaled, dtype=torch.float32, device=device),\n",
    "    'train_label': F.one_hot(torch.tensor(train_y, dtype=torch.long, device=device), num_classes=6),\n",
    "    'test_input': torch.tensor(test_X_scaled, dtype=torch.float32, device=device),\n",
    "    'test_label': F.one_hot(torch.tensor(test_y, dtype=torch.long, device=device), num_classes=6)\n",
    "}\n",
    "print(\"Data prepared.\",\n",
    "      f\"Train set: {dataset['train_input'].shape, dataset['train_label'].shape}\",\n",
    "      f\"Test set: {dataset['test_input'].shape, dataset['test_label'].shape}\", sep=\"\\n\")\n",
    "\n",
    "# Define a dataset the label is numeric\n",
    "dataset_numeric = {\n",
    "    'train_input': torch.tensor(train_X_scaled, dtype=torch.float32, device=device),\n",
    "    'train_label': torch.tensor(train_y, dtype=torch.long, device=device),\n",
    "    'test_input': torch.tensor(test_X_scaled, dtype=torch.float32, device=device),\n",
    "    'test_label': torch.tensor(test_y, dtype=torch.long, device=device)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(loader, model):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data, labels in loader:\n",
    "        logits = model(data)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.numel()\n",
    "    return total_correct.float() / total_samples\n",
    "\n",
    "# Define the DataLoader adapted to the LTN input format. 'data' is same, 'labels' is numeric (not one-hot)\n",
    "train_loader = DataLoader(\n",
    "    data=dataset['train_input'], \n",
    "    labels=torch.tensor(train_y, dtype=torch.long, device=device), \n",
    "    batch_size=len(dataset['train_input']))\n",
    "test_loader = DataLoader(\n",
    "    data=dataset['test_input'], \n",
    "    labels=torch.tensor(test_y, dtype=torch.long, device=device), \n",
    "    batch_size=len(dataset['test_input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ltn\n",
    "import ltn.fuzzy_ops\n",
    "\n",
    "# define the connectives, quantifiers, and the SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())   # And = ltn.Connective(custom_fuzzy_ops.AndProd())\n",
    "Or = ltn.Connective(ltn.fuzzy_ops.OrProbSum())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "Exists = ltn.Quantifier(ltn.fuzzy_ops.AggregPMean(p=2), quantifier=\"e\")\n",
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()\n",
    "\n",
    "# define ltn constants\n",
    "l_MQTT_DDoS_Connect_Flood = ltn.Constant(torch.tensor([1, 0, 0, 0, 0, 0]))\n",
    "l_MQTT_DDoS_Publish_Flood = ltn.Constant(torch.tensor([0, 1, 0, 0, 0, 0]))\n",
    "l_MQTT_DoS_Connect_Flood = ltn.Constant(torch.tensor([0, 0, 1, 0, 0, 0]))\n",
    "l_MQTT_DoS_Publish_Flood = ltn.Constant(torch.tensor([0, 0, 0, 1, 0, 0]))\n",
    "l_MQTT_Malformed_Data = ltn.Constant(torch.tensor([0, 0, 0, 0, 1, 0]))\n",
    "l_Benign = ltn.Constant(torch.tensor([0, 0, 0, 0, 0, 1]))\n",
    "\n",
    "# utils\n",
    "import re\n",
    "\n",
    "parse_mlp = lambda line: (\n",
    "    float(re.search(r\"Loss:\\s*([\\d\\.]+)\", line).group(1)),\n",
    "    float(re.search(r\"Test accuracy:\\s*([\\d\\.]+)\", line).group(1))\n",
    ")\n",
    "\n",
    "parse_kan = lambda line: (\n",
    "    float(re.search(r\"Epoch\\s*\\d+\\s*\\|\\s*KAN\\s*\\(loss/acc/sat\\):\\s*([\\d\\.]+)\", line).group(1)),\n",
    "    float(re.search(r\"KAN\\s*\\(loss/acc/sat\\):\\s*[\\d\\.]+/([\\d\\.]+)\", line).group(1)),\n",
    "    float(re.search(r\"KAN\\s*\\(loss/acc/sat\\):\\s*[\\d\\.]+/[\\d\\.]+/([\\d\\.]+)\", line).group(1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL-h Plot**\n",
    "\n",
    "Setting: [input(18), 6, 6, output(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.33e+00 | test_loss: 4.06e+00 | reg: 3.49e+01 | : 100%|â–ˆ| 1/1 [00:01<00:00,  1.07s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model version 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train_loss': [array(1.3342661, dtype=float32)],\n",
       " 'test_loss': [array(4.061073, dtype=float32)],\n",
       " 'reg': [array(34.86599, dtype=float32)]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_sat_levels(loader, P):\n",
    "\tsat_level  = 0\n",
    "\tfor data, labels in loader:\n",
    "\t\tx = ltn.Variable(\"x\", data)\n",
    "\t\tx_MQTT_DDoS_Connect_Flood = ltn.Variable(\"x_MQTT_DDoS_Connect_Flood\", data[labels == 0])\n",
    "\t\tx_MQTT_DDoS_Publish_Flood = ltn.Variable(\"x_MQTT_DDoS_Publish_Flood\", data[labels == 1])\n",
    "\t\tx_MQTT_DoS_Connect_Flood = ltn.Variable(\"x_MQTT_DoS_Connect_Flood\", data[labels == 2])\n",
    "\t\tx_MQTT_DoS_Publish_Flood = ltn.Variable(\"x_MQTT_DoS_Publish_Flood\", data[labels == 3])\n",
    "\t\tx_MQTT_Malformed_Data = ltn.Variable(\"x_MQTT_Malformed_Data\", data[labels == 4])\n",
    "\t\tx_Benign = ltn.Variable(\"x_Benign\", data[labels == 5])\n",
    "\n",
    "\t\tx_MQTT = ltn.Variable(\"x_MQTT\", data[labels < 5])\n",
    "\t\t\n",
    "\t\tsat_level = SatAgg(\n",
    "\t\t\tForall(x_MQTT_DDoS_Connect_Flood, P(x_MQTT_DDoS_Connect_Flood, l_MQTT_DDoS_Connect_Flood)),\n",
    "\t\t\tForall(x_MQTT_DDoS_Publish_Flood, P(x_MQTT_DDoS_Publish_Flood, l_MQTT_DDoS_Publish_Flood)),\n",
    "\t\t\tForall(x_MQTT_DoS_Connect_Flood, P(x_MQTT_DoS_Connect_Flood, l_MQTT_DoS_Connect_Flood)),\n",
    "\t\t\tForall(x_MQTT_DoS_Publish_Flood, P(x_MQTT_DoS_Publish_Flood, l_MQTT_DoS_Publish_Flood)),\n",
    "\t\t\tForall(x_MQTT_Malformed_Data, P(x_MQTT_Malformed_Data, l_MQTT_Malformed_Data)),\n",
    "\t\t\tForall(x_Benign, P(x_Benign, l_Benign)),\n",
    "\t\t\t# hierarchical constraints\n",
    "\t\t\tForall(x_MQTT, Not(P(x_MQTT, l_Benign)))\n",
    "\t\t)\n",
    "\treturn sat_level\n",
    "    \n",
    "kan_h = KAN(width=[18, 6, 6, 6], grid=5, k=3, seed=42, device=device)\n",
    "P_kan = ltn.Predicate(LogitsToPredicate(MultiKANModel(kan_h)))\n",
    "\n",
    "# optimizer_kan = torch.optim.Adam(P_kan.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "kan_h.fit(dataset_numeric, steps=1, opt=\"Adam\", loss_fn=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem:\n",
    "* MultiKAN.fit() encapsulate only two optimizer, and can't switch loss_fn neatly\n",
    "* use my code to train, need a way to SAVE THE MODEL, then LOAD MODEL to plot (MultiKAN.plot())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
