{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef5405d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch\n",
    "from kan import KAN\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    \"CUDA initialization: Unexpected error from cudaGetDeviceCount\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d743c59",
   "metadata": {},
   "source": [
    "##### Loading the inference model and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e96dba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Using device: cpu \n",
      "\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n",
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    }
   ],
   "source": [
    "################################setup######################################\n",
    "def load_csv_data(input_folder: str,\n",
    "                  train_fname: str,\n",
    "                  test_fname: str):\n",
    "    \"\"\"\n",
    "    Reads train & test CSVs from disk.\n",
    "    \n",
    "    Returns:\n",
    "      train_df, test_df (both pandas.DataFrame)\n",
    "    \"\"\"\n",
    "    train_path = os.path.join(input_folder, train_fname)\n",
    "    test_path  = os.path.join(input_folder, test_fname)\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def extract_features_labels(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into numpy feature array X and label vector y.\n",
    "    \n",
    "    The last column is the label.\n",
    "    \"\"\"\n",
    "    X = df.iloc[:, :-1].values\n",
    "    y = df.iloc[:,  -1].values\n",
    "    return X, y\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "            ############################################################\n",
    "            # Check if any class is missing in the batch\n",
    "            # present_classes = np.unique(labels.cpu().numpy())\n",
    "            # all_classes = np.arange(len(label_mapping))  # Adjust based on number of classes\n",
    "            # missing_classes = set(all_classes) - set(present_classes)\n",
    "            #\n",
    "            # if missing_classes:\n",
    "            #     print(f\"Batch {start_idx // self.batch_size} is missing classes {missing_classes}\")\n",
    "            ############################################################\n",
    "            yield data, labels\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)  # 计算并返回与给定类标签l对应的概率值\n",
    "        return out\n",
    "\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logits for the classes given an input example. It does not compute the softmax, so the output\n",
    "    are not normalized.\n",
    "    This is done to separate the accuracy computation from the satisfaction level computation. Go through the example\n",
    "    to understand it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Method which defines the forward phase of the neural network for our multi class classification task.\n",
    "        In particular, it returns the logits for the classes given an input example.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :param training: whether the network is in training mode (dropout applied) or validation mode (dropout not applied)\n",
    "        :return: logits for example x\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class MultiKANModel(torch.nn.Module):\n",
    "    def __init__(self, kan):\n",
    "        \"\"\"\n",
    "        Wrap an already built MultKAN instance.\n",
    "        Args:\n",
    "            kan: a MultKAN model (which has attributes such as act_fun, symbolic_fun, node_bias, node_scale,\n",
    "                 subnode_bias, subnode_scale, depth, width, mult_homo, mult_arity, input_id, symbolic_enabled, etc.)\n",
    "        \"\"\"\n",
    "        super(MultiKANModel, self).__init__()\n",
    "        self.kan = kan\n",
    "\n",
    "    def forward(self, x, training=False, singularity_avoiding=False, y_th=10.):\n",
    "        # Select input features according to input_id\n",
    "        x = x[:, self.kan.input_id.long()]\n",
    "        # Loop through each layer\n",
    "        for l in range(self.kan.depth):\n",
    "            # Get outputs from the numerical branch (KANLayer) of current layer\n",
    "            x_numerical, preacts, postacts_numerical, postspline = self.kan.act_fun[l](x)\n",
    "            # Get output from the symbolic branch if enabled\n",
    "            if self.kan.symbolic_enabled:\n",
    "                x_symbolic, postacts_symbolic = self.kan.symbolic_fun[l](x, singularity_avoiding=singularity_avoiding, y_th=y_th)\n",
    "            else:\n",
    "                x_symbolic = 0.\n",
    "            # Sum the numerical and symbolic outputs\n",
    "            x = x_numerical + x_symbolic\n",
    "\n",
    "            # Subnode affine transformation\n",
    "            x = self.kan.subnode_scale[l][None, :] * x + self.kan.subnode_bias[l][None, :]\n",
    "\n",
    "            # Process multiplication nodes\n",
    "            dim_sum = self.kan.width[l+1][0]\n",
    "            dim_mult = self.kan.width[l+1][1]\n",
    "            if dim_mult > 0:\n",
    "                if self.kan.mult_homo:\n",
    "                    for i in range(self.kan.mult_arity-1):\n",
    "                        if i == 0:\n",
    "                            x_mult = x[:, dim_sum::self.kan.mult_arity] * x[:, dim_sum+1::self.kan.mult_arity]\n",
    "                        else:\n",
    "                            x_mult = x_mult * x[:, dim_sum+i+1::self.kan.mult_arity]\n",
    "                else:\n",
    "                    for j in range(dim_mult):\n",
    "                        acml_id = dim_sum + int(np.sum(self.kan.mult_arity[l+1][:j]))\n",
    "                        for i in range(self.kan.mult_arity[l+1][j]-1):\n",
    "                            if i == 0:\n",
    "                                x_mult_j = x[:, [acml_id]] * x[:, [acml_id+1]]\n",
    "                            else:\n",
    "                                x_mult_j = x_mult_j * x[:, [acml_id+i+1]]\n",
    "                        if j == 0:\n",
    "                            x_mult = x_mult_j\n",
    "                        else:\n",
    "                            x_mult = torch.cat([x_mult, x_mult_j], dim=1)\n",
    "                # Concatenate sum and mult parts\n",
    "                x = torch.cat([x[:, :dim_sum], x_mult], dim=1)\n",
    "\n",
    "            # Node affine transformation\n",
    "            x = self.kan.node_scale[l][None, :] * x + self.kan.node_bias[l][None, :]\n",
    "\n",
    "        # Final x corresponds to the logits output of the whole model\n",
    "        return x\n",
    "\n",
    "\n",
    "def save_model(model, model_save_folder, model_name):\n",
    "    \"\"\"\n",
    "    Save the model to disk.\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), os.path.join(model_save_folder, model_name))\n",
    "\n",
    "    print(f\"Model saved to {os.path.join(model_save_folder, model_name)}\")\n",
    "\n",
    "\n",
    "def load_model_state(infer_model, model_save_folder, model_name):\n",
    "    \"\"\"\n",
    "    Load the model from disk.\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(\n",
    "        os.path.join(model_save_folder, model_name),\n",
    "        map_location=device,\n",
    "        weights_only=True     # <-- only load tensor weights, no pickle objects\n",
    "    )\n",
    "    infer_model.load_state_dict(checkpoint)\n",
    "    infer_model.eval()\n",
    "    return infer_model\n",
    "\n",
    "\n",
    "def compute_accuracy(loader, model):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for data, labels in loader:\n",
    "        logits = model(data)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        total_correct += (preds == labels).sum()\n",
    "        total_samples += labels.numel()\n",
    "    return total_correct.float() / total_samples\n",
    "\n",
    "\n",
    "def compute_sat_levels(loader, P):\n",
    "\tsat_level  = 0\n",
    "\tfor data, labels in loader:\n",
    "\t\tx = ltn.Variable(\"x\", data)\n",
    "\t\tx_MQTT_DDoS_Connect_Flood = ltn.Variable(\"x_MQTT_DDoS_Connect_Flood\", data[labels == 0])\n",
    "\t\tx_MQTT_DDoS_Publish_Flood = ltn.Variable(\"x_MQTT_DDoS_Publish_Flood\", data[labels == 1])\n",
    "\t\tx_MQTT_DoS_Connect_Flood = ltn.Variable(\"x_MQTT_DoS_Connect_Flood\", data[labels == 2])\n",
    "\t\tx_MQTT_DoS_Publish_Flood = ltn.Variable(\"x_MQTT_DoS_Publish_Flood\", data[labels == 3])\n",
    "\t\tx_MQTT_Malformed_Data = ltn.Variable(\"x_MQTT_Malformed_Data\", data[labels == 4])\n",
    "\t\tx_Benign = ltn.Variable(\"x_Benign\", data[labels == 5])\n",
    "\n",
    "\t\tsat_level = SatAgg(\n",
    "\t\t\tForall(x_MQTT_DDoS_Connect_Flood, P(x_MQTT_DDoS_Connect_Flood, l_MQTT_DDoS_Connect_Flood)),\n",
    "\t\t\tForall(x_MQTT_DDoS_Publish_Flood, P(x_MQTT_DDoS_Publish_Flood, l_MQTT_DDoS_Publish_Flood)),\n",
    "\t\t\tForall(x_MQTT_DoS_Connect_Flood, P(x_MQTT_DoS_Connect_Flood, l_MQTT_DoS_Connect_Flood)),\n",
    "\t\t\tForall(x_MQTT_DoS_Publish_Flood, P(x_MQTT_DoS_Publish_Flood, l_MQTT_DoS_Publish_Flood)),\n",
    "\t\t\tForall(x_MQTT_Malformed_Data, P(x_MQTT_Malformed_Data, l_MQTT_Malformed_Data)),\n",
    "\t\t\tForall(x_Benign, P(x_Benign, l_Benign))\n",
    "\t\t)\n",
    "\treturn sat_level\n",
    "\n",
    "\n",
    "##############################Load data######################################\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")  # Use CPU for this example\n",
    "print(f\"\\n Using device: {device} \\n\")\n",
    "\n",
    "# Load data\n",
    "input_folder = '/home/zyang44/Github/baseline_cicIOT/P1_structurelevel/efficiency/input_files'\n",
    "train_fname = 'logiKNet_train_35945.csv'\n",
    "test_fname = 'logiKNet_test_3994.csv'\n",
    "\n",
    "train_df, test_df = load_csv_data(input_folder, train_fname, test_fname)\n",
    "# Extract features and labels   \n",
    "X_train, y_train = extract_features_labels(train_df)\n",
    "X_test, y_test = extract_features_labels(test_df)\n",
    "\n",
    "dataset_numeric = {\n",
    "    'train_input': torch.tensor(X_train, dtype=torch.float32, device=device),\n",
    "    'train_label': torch.tensor(y_train, dtype=torch.long, device=device),\n",
    "    'test_input': torch.tensor(X_test, dtype=torch.float32, device=device),\n",
    "    'test_label': torch.tensor(y_test, dtype=torch.long, device=device)\n",
    "}\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset_numeric['train_input'],\n",
    "    dataset_numeric['train_label'], \n",
    "    batch_size=len(X_train), \n",
    "    shuffle=True\n",
    "    )\n",
    "test_loader = DataLoader(\n",
    "    dataset_numeric['test_input'],\n",
    "    dataset_numeric['test_label'],\n",
    "    # batch_size=32,\n",
    "    shuffle=False\n",
    "    )\n",
    "\n",
    "\n",
    "###############################load model and testing########################################\n",
    "model_state_folder = '/home/zyang44/Github/baseline_cicIOT/P1_structurelevel/efficiency/model_weights'\n",
    "\n",
    "# load all four models\n",
    "mlp_infer = MLP(layer_sizes=(18, 10, 6)).to(device)\n",
    "mlp_infer = load_model_state(mlp_infer, model_state_folder, 'mlp.pt')\n",
    "\n",
    "logicmlp_infer = MLP(layer_sizes=(18, 10, 6)).to(device)\n",
    "logicmlp_infer = load_model_state(logicmlp_infer, model_state_folder, 'logic_mlp.pt')\n",
    "\n",
    "logiKNet_infer = KAN(width=[18, 10, 6], grid=5, k=3, seed=42, device=device)\n",
    "logiKNet_infer = load_model_state(logiKNet_infer, model_state_folder, 'logiKNet.pt')\n",
    "\n",
    "hierarchical_logiKNet_infer = KAN(width=[18, 10, 6], grid=5, k=3, seed=42, device=device)\n",
    "hierarchical_logiKNet_infer = load_model_state(hierarchical_logiKNet_infer, model_state_folder, 'hierarchical_logiKNet.pt')\n",
    "\n",
    "model_list = {\n",
    "    'mlp': mlp_infer,\n",
    "    'logic_mlp': logicmlp_infer,\n",
    "    'logiKNet': logiKNet_infer,\n",
    "    'hierarchical_logiKNet': hierarchical_logiKNet_infer\n",
    "}\n",
    "\n",
    "# test the models \n",
    "def test_model(model, loader, model_name=\"\"):\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    model.eval()\n",
    "    batch_times = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            batch_start = time.perf_counter()\n",
    "            logits = model(data)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            batch_end = time.perf_counter()\n",
    "            batch_times.append(batch_end - batch_start)\n",
    "\n",
    "    if batch_times:\n",
    "        mean_time = sum(batch_times) / len(batch_times)\n",
    "        print(f\"[{model_name}] Mean batch inference time: {mean_time:.4f} seconds\")\n",
    "    else:\n",
    "        print(f\"[{model_name}] No batches to measure.\") \n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    print(f\"[{model_name}] Inference time: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "# for model_name, model in model_list.items():\n",
    "#     test_model(model, test_loader, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732432a1",
   "metadata": {},
   "source": [
    "#### Persistent Processing Power vs Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0fbc7e",
   "metadata": {},
   "source": [
    "##### Statistics for mean inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c149f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "jetson_stat = {\n",
    "    'Inference': [0.68151165, 0.67085658, 159.49545998, 158.91916464],\n",
    "    'Power': [1460.3, 1340.9, 1034.2, 1042.2],\n",
    "    'Background Power': 4850\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28426d",
   "metadata": {},
   "source": [
    "- **For mlp/logic_mlp**: set unit time to 1 ms.\n",
    "- **For logiKNet/h-logiKNet**: set unit time to 200 ms.\n",
    "\n",
    "- Persistent: detection software always running, the mean power consumption is: \n",
    "  **jetson_stat['Backgournd Power'] + jetson_stat['Power']**\n",
    "- Intermittent: dectect software run every 5 unit of flow time, the mean power consumption is:\n",
    "  **(5 * jetson_stat['Backgournd Power'] + jetson_stat['Inference'] * jetson_stat['Power']) / 5**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e391f5e",
   "metadata": {},
   "source": [
    "#### Calculate Mean Power Comsumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c00a92f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Consumption Analysis (mW)\n",
      "============================================================\n",
      "Model                Persistent   Intermittent Savings   \n",
      "------------------------------------------------------------\n",
      "mlp                  6310.3       5049.0       20.0      %\n",
      "logic_mlp            6190.9       5029.9       18.8      %\n",
      "logiKNet             5884.2       5015.0       14.8      %\n",
      "hierarchical_logiKNet 5892.2       5015.6       14.9      %\n",
      "\n",
      "Power Consumption Dictionary:\n",
      "{'persistent': {'mlp': 6310.3, 'logic_mlp': 6190.9, 'logiKNet': 5884.2, 'hierarchical_logiKNet': 5892.2}, 'intermittent': {'mlp': 5049.042292499, 'logic_mlp': 5029.910317624401, 'logiKNet': 5014.9502047113165, 'hierarchical_logiKNet': 5015.6255533878075}}\n"
     ]
    }
   ],
   "source": [
    "# Calculate the mean power consumption for persistent and intermittent modes\n",
    "model_names = ['mlp', 'logic_mlp', 'logiKNet', 'hierarchical_logiKNet']\n",
    "\n",
    "# Initialize dictionary to store power consumption results\n",
    "power_consumption = {\n",
    "    'persistent': {},\n",
    "    'intermittent': {}\n",
    "}\n",
    "\n",
    "n = 5  # Intermittent mode runs every n units of flow time\n",
    "\n",
    "# Calculate power consumption for each model\n",
    "for i, model_name in enumerate(model_names):\n",
    "    # Persistent mode: detection software always running\n",
    "    # Mean power = Background Power + Power\n",
    "    persistent_power = jetson_stat['Background Power'] + jetson_stat['Power'][i]\n",
    "    \n",
    "    # Intermittent mode: detection software runs every n units of flow time\n",
    "    # Mean power = (n * Background Power + Inference * Power) / n\n",
    "    if model_name == 'mlp' or model_name == 'logic_mlp':\n",
    "        intermittent_power = (n * jetson_stat['Background Power'] +\n",
    "                             jetson_stat['Inference'][i] * jetson_stat['Power'][i]) / n\n",
    "    else:\n",
    "        # For logiKNet and hierarchical_logiKNet, use the same formula\n",
    "        # Mean power = (200n * Background Power + Inference * Power) / 200n\n",
    "        intermittent_power = (200 * n * jetson_stat['Background Power'] +\n",
    "                             jetson_stat['Inference'][i] * jetson_stat['Power'][i]) / (200 * n)\n",
    "\n",
    "    # Store results\n",
    "    power_consumption['persistent'][model_name] = persistent_power\n",
    "    power_consumption['intermittent'][model_name] = intermittent_power\n",
    "\n",
    "# Display results\n",
    "print(\"Power Consumption Analysis (mW)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<20} {'Persistent':<12} {'Intermittent':<12} {'Savings':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name in model_names:\n",
    "    persistent = power_consumption['persistent'][model_name]\n",
    "    intermittent = power_consumption['intermittent'][model_name]\n",
    "    savings = ((persistent - intermittent) / persistent) * 100\n",
    "    \n",
    "    print(f\"{model_name:<20} {persistent:<12.1f} {intermittent:<12.1f} {savings:<10.1f}%\")\n",
    "\n",
    "print(\"\\nPower Consumption Dictionary:\")\n",
    "print(power_consumption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a866e87",
   "metadata": {},
   "source": [
    "#### Calculate Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa01eec",
   "metadata": {},
   "source": [
    "- The unit time must longer than model single inference duration. Because we didn't have multi-task running power consumption.\n",
    "- The threat rate is to see how many malicious case missed, two options here: **false negative Benign is counted** or not.\n",
    "\n",
    "{\"MQTT-DDoS-Connect_Flood\": 0, \n",
    "\"MQTT-DDoS-Publish_Flood\": 1, \n",
    "\"MQTT-DoS-Connect_Flood\": 2, \n",
    "\"MQTT-DoS-Publish_Flood\": 3, \n",
    "\"MQTT-Malformed_Data\": 4, \n",
    "\"Benign\": 5} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c32500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Comprehensive Threat Rate Analysis...\n",
      "Testing intermit intervals: [1, 2, 3, 5]\n",
      "- Interval 1 = Persistent mode (all cases processed)\n",
      "- Intervals 2,3,5 = Intermittent mode (sampled cases only)\n",
      "\n",
      "\n",
      "Comprehensive Threat Rate Analysis\n",
      "================================================================================\n",
      "\n",
      "Analyzing Intermit Interval: 1\n",
      "--------------------------------------------------\n",
      "mlp                  | Persistent                | Threat Rate: 0.4421 (44.21%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic_mlp            | Persistent                | Threat Rate: 0.3871 (38.71%)\n",
      "logiKNet             | Persistent                | Threat Rate: 0.2440 (24.40%)\n",
      "hierarchical_logiKNet | Persistent                | Threat Rate: 0.2349 (23.49%)\n",
      "\n",
      "Analyzing Intermit Interval: 2\n",
      "--------------------------------------------------\n",
      "mlp                  | Intermittent (interval=2) | Threat Rate: 0.7226 (72.26%)\n",
      "                     |                           | Sampled: 1998/3994 (50.0%)\n",
      "logic_mlp            | Intermittent (interval=2) | Threat Rate: 0.6937 (69.37%)\n",
      "                     |                           | Sampled: 1998/3994 (50.0%)\n",
      "logiKNet             | Intermittent (interval=2) | Threat Rate: 0.6244 (62.44%)\n",
      "                     |                           | Sampled: 1998/3994 (50.0%)\n",
      "hierarchical_logiKNet | Intermittent (interval=2) | Threat Rate: 0.6205 (62.05%)\n",
      "                     |                           | Sampled: 1998/3994 (50.0%)\n",
      "\n",
      "Analyzing Intermit Interval: 3\n",
      "--------------------------------------------------\n",
      "mlp                  | Intermittent (interval=3) | Threat Rate: 0.8143 (81.43%)\n",
      "                     |                           | Sampled: 1332/3994 (33.4%)\n",
      "logic_mlp            | Intermittent (interval=3) | Threat Rate: 0.7952 (79.52%)\n",
      "                     |                           | Sampled: 1332/3994 (33.4%)\n",
      "logiKNet             | Intermittent (interval=3) | Threat Rate: 0.7469 (74.69%)\n",
      "                     |                           | Sampled: 1332/3994 (33.4%)\n",
      "hierarchical_logiKNet | Intermittent (interval=3) | Threat Rate: 0.7438 (74.38%)\n",
      "                     |                           | Sampled: 1332/3994 (33.4%)\n",
      "\n",
      "Analyzing Intermit Interval: 5\n",
      "--------------------------------------------------\n",
      "mlp                  | Intermittent (interval=5) | Threat Rate: 0.8888 (88.88%)\n",
      "                     |                           | Sampled: 800/3994 (20.0%)\n",
      "logic_mlp            | Intermittent (interval=5) | Threat Rate: 0.8800 (88.00%)\n",
      "                     |                           | Sampled: 800/3994 (20.0%)\n",
      "logiKNet             | Intermittent (interval=5) | Threat Rate: 0.8499 (84.99%)\n",
      "                     |                           | Sampled: 800/3994 (20.0%)\n",
      "hierarchical_logiKNet | Intermittent (interval=5) | Threat Rate: 0.8481 (84.81%)\n",
      "                     |                           | Sampled: 800/3994 (20.0%)\n",
      "\n",
      "====================================================================================================\n",
      "THREAT RATE COMPARISON TABLE\n",
      "====================================================================================================\n",
      "Model                | Persistent   | Interval=2   | Interval=3   | Interval=5   | Max Increase\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mlp                  | 0.4421 | 0.7226 | 0.8143 | 0.8888 |      101.0%\n",
      "logic_mlp            | 0.3871 | 0.6937 | 0.7952 | 0.8800 |      127.3%\n",
      "logiKNet             | 0.2440 | 0.6244 | 0.7469 | 0.8499 |      248.3%\n",
      "hierarchical_logiKNet | 0.2349 | 0.6205 | 0.7438 | 0.8481 |      261.1%\n",
      "Results saved to: threat_rate_analysis.txt\n",
      "\n",
      "Final Results Dictionary:\n",
      "==================================================\n",
      "Interval 1:\n",
      "  mlp: 0.442115\n",
      "  logic_mlp: 0.387116\n",
      "  logiKNet: 0.243999\n",
      "  hierarchical_logiKNet: 0.234883\n",
      "\n",
      "Interval 2:\n",
      "  mlp: 0.722577\n",
      "  logic_mlp: 0.693710\n",
      "  logiKNet: 0.624430\n",
      "  hierarchical_logiKNet: 0.620480\n",
      "\n",
      "Interval 3:\n",
      "  mlp: 0.814342\n",
      "  logic_mlp: 0.795199\n",
      "  logiKNet: 0.746885\n",
      "  hierarchical_logiKNet: 0.743847\n",
      "\n",
      "Interval 5:\n",
      "  mlp: 0.888788\n",
      "  logic_mlp: 0.879976\n",
      "  logiKNet: 0.849894\n",
      "  hierarchical_logiKNet: 0.848070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE THREAT RATE ANALYSIS WITH INTERMITTENT INFERENCE\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_intermittent_threat_rate(model, test_loader, intermit_interval=2):\n",
    "    \"\"\"\n",
    "    Calculate the threat rate of a model with intermittent inference.\n",
    "    Threat rate is calculated based on ALL test cases, but only sampled cases get predictions.\n",
    "    All non-sampled cases are treated as missed (false negatives).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get all test data\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for data, labels in test_loader:\n",
    "        all_data.append(data)\n",
    "        all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_data = torch.cat(all_data, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    \n",
    "    total_cases = len(all_data)\n",
    "    \n",
    "    # Initialize predictions array - all start as \"missed\" (predict as Benign=5)\n",
    "    all_preds = np.full(total_cases, 5)  # Default to Benign class\n",
    "    sampled_indices = []\n",
    "    \n",
    "    # Handle persistent mode (intermit_interval=1)\n",
    "    if intermit_interval == 1:\n",
    "        # Process all cases (persistent mode)\n",
    "        with torch.no_grad():\n",
    "            logits = model(all_data)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds = preds.cpu().numpy()\n",
    "            sampled_indices = list(range(total_cases))\n",
    "    else:\n",
    "        # Intermittent mode - Do inference on case 0 as initialization\n",
    "        if total_cases > 0:\n",
    "            with torch.no_grad():\n",
    "                logits = model(all_data[0:1])\n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                all_preds[0] = pred.cpu().numpy()[0]\n",
    "                sampled_indices.append(0)\n",
    "        \n",
    "        # Process remaining cases in groups\n",
    "        current_idx = 1\n",
    "        while current_idx < total_cases:\n",
    "            group_end = min(current_idx + intermit_interval, total_cases)\n",
    "            group_indices = list(range(current_idx, group_end))\n",
    "            \n",
    "            if len(group_indices) > 0:\n",
    "                selected_idx = np.random.choice(group_indices)\n",
    "                sampled_indices.append(selected_idx)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    logits = model(all_data[selected_idx:selected_idx+1])\n",
    "                    pred = torch.argmax(logits, dim=1)\n",
    "                    all_preds[selected_idx] = pred.cpu().numpy()[0]\n",
    "            \n",
    "            current_idx = group_end\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = all_labels.cpu().numpy()\n",
    "    \n",
    "    # Calculate threat rate on ALL cases\n",
    "    malicious_mask = all_labels != 5\n",
    "    malicious_labels = all_labels[malicious_mask]\n",
    "    malicious_preds = all_preds[malicious_mask]\n",
    "    \n",
    "    missed_malicious = np.sum(malicious_labels != malicious_preds)\n",
    "    total_malicious = len(malicious_labels)\n",
    "    \n",
    "    if total_malicious == 0:\n",
    "        return 0.0, {}\n",
    "    \n",
    "    threat_rate = missed_malicious / total_malicious\n",
    "    \n",
    "    # Calculate sampling statistics\n",
    "    sampling_stats = {\n",
    "        'total_cases': total_cases,\n",
    "        'sampled_cases': len(sampled_indices),\n",
    "        'sampling_rate': len(sampled_indices) / total_cases,\n",
    "        'total_malicious': total_malicious,\n",
    "        'missed_malicious': missed_malicious\n",
    "    }\n",
    "    \n",
    "    return threat_rate, sampling_stats\n",
    "\n",
    "\n",
    "def run_comprehensive_threat_analysis(model_list, test_loader, intermit_intervals=[1, 2, 3, 5]):\n",
    "    \"\"\"Run comprehensive threat rate analysis for different intermit intervals.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Comprehensive Threat Rate Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for interval in intermit_intervals:\n",
    "        print(f\"\\nAnalyzing Intermit Interval: {interval}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        results[interval] = {}\n",
    "        \n",
    "        for model_name, model in model_list.items():\n",
    "            # Set random seed for reproducibility\n",
    "            np.random.seed(42)\n",
    "            \n",
    "            threat_rate, stats = calculate_intermittent_threat_rate(\n",
    "                model, test_loader, intermit_interval=interval\n",
    "            )\n",
    "            \n",
    "            results[interval][model_name] = {\n",
    "                'threat_rate': threat_rate,\n",
    "                'sampling_stats': stats\n",
    "            }\n",
    "            \n",
    "            mode_str = \"Persistent\" if interval == 1 else f\"Intermittent (interval={interval})\"\n",
    "            print(f\"{model_name:<20} | {mode_str:<25} | Threat Rate: {threat_rate:.4f} ({threat_rate*100:.2f}%)\")\n",
    "            \n",
    "            if interval > 1:\n",
    "                print(f\"{'':>20} | {'':>25} | Sampled: {stats['sampled_cases']}/{stats['total_cases']} ({stats['sampling_rate']*100:.1f}%)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def display_comparison_table(results, intermit_intervals=[1, 2, 3, 5]):\n",
    "    \"\"\"Display a comprehensive comparison table of threat rates.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(\"THREAT RATE COMPARISON TABLE\")\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # Header\n",
    "    header = f\"{'Model':<20}\"\n",
    "    for interval in intermit_intervals:\n",
    "        mode_str = \"Persistent\" if interval == 1 else f\"Interval={interval}\"\n",
    "        header += f\" | {mode_str:<12}\"\n",
    "    header += \" | Max Increase\"\n",
    "    print(header)\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    # Data rows\n",
    "    for model_name in results[intermit_intervals[0]].keys():\n",
    "        row = f\"{model_name:<20}\"\n",
    "        threat_rates = []\n",
    "        \n",
    "        for interval in intermit_intervals:\n",
    "            threat_rate = results[interval][model_name]['threat_rate']\n",
    "            threat_rates.append(threat_rate)\n",
    "            row += f\" | {threat_rate:.4f}\"\n",
    "        \n",
    "        # Calculate maximum increase from persistent mode\n",
    "        persistent_rate = threat_rates[0]  # interval=1 is persistent\n",
    "        max_increase = max([(rate - persistent_rate) / persistent_rate * 100 \n",
    "                          for rate in threat_rates[1:]] if len(threat_rates) > 1 else [0])\n",
    "        \n",
    "        row += f\" | {max_increase:>10.1f}%\"\n",
    "        print(row)\n",
    "\n",
    "\n",
    "def save_results_to_file(results, filename='threat_rate_analysis.txt'):\n",
    "    \"\"\"Save the comprehensive results to a text file.\"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Comprehensive Threat Rate Analysis Results\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for interval in sorted(results.keys()):\n",
    "            f.write(f\"Intermit Interval: {interval}\\n\")\n",
    "            f.write(\"-\" * 30 + \"\\n\")\n",
    "            \n",
    "            for model_name, data in results[interval].items():\n",
    "                threat_rate = data['threat_rate']\n",
    "                stats = data['sampling_stats']\n",
    "                \n",
    "                f.write(f\"Model: {model_name}\\n\")\n",
    "                f.write(f\"  Threat Rate: {threat_rate:.6f} ({threat_rate*100:.2f}%)\\n\")\n",
    "                f.write(f\"  Total Cases: {stats['total_cases']}\\n\")\n",
    "                f.write(f\"  Sampled Cases: {stats['sampled_cases']}\\n\")\n",
    "                f.write(f\"  Sampling Rate: {stats['sampling_rate']:.4f} ({stats['sampling_rate']*100:.1f}%)\\n\")\n",
    "                f.write(f\"  Total Malicious: {stats['total_malicious']}\\n\")\n",
    "                f.write(f\"  Missed Malicious: {stats['missed_malicious']}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Results saved to: {filename}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "# Check if this is the first run to avoid duplicate execution\n",
    "import sys\n",
    "if not hasattr(sys, '_threat_analysis_executed'):\n",
    "    print(\"Starting Comprehensive Threat Rate Analysis...\")\n",
    "    print(\"Testing intermit intervals: [1, 2, 3, 5]\")\n",
    "    print(\"- Interval 1 = Persistent mode (all cases processed)\")\n",
    "    print(\"- Intervals 2,3,5 = Intermittent mode (sampled cases only)\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Run the comprehensive analysis\n",
    "    intermit_intervals = [1, 2, 3, 5]\n",
    "    results = run_comprehensive_threat_analysis(model_list, test_loader, intermit_intervals)\n",
    "\n",
    "    # Display comparison table\n",
    "    display_comparison_table(results, intermit_intervals)\n",
    "\n",
    "    # Save results to file\n",
    "    save_results_to_file(results, 'threat_rate_analysis.txt')\n",
    "\n",
    "    print(f\"\\nFinal Results Dictionary:\")\n",
    "    print(\"=\" * 50)\n",
    "    for interval in intermit_intervals:\n",
    "        print(f\"Interval {interval}:\")\n",
    "        for model_name in results[interval]:\n",
    "            threat_rate = results[interval][model_name]['threat_rate']\n",
    "            print(f\"  {model_name}: {threat_rate:.6f}\")\n",
    "        print()\n",
    "    \n",
    "    # Mark as executed to prevent duplicate runs\n",
    "    sys._threat_analysis_executed = True\n",
    "    \n",
    "else:\n",
    "    print(\"Analysis already executed. Results are available in the 'results' variable.\")\n",
    "    print(\"To run again, restart the kernel or delete the execution flag with:\")\n",
    "    print(\"del sys._threat_analysis_executed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LTN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
