{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class single label classification\n",
    "\n",
    "The natural extension of binary classification is a multi-class classification task.\n",
    "\n",
    "We first approach multi-class single-label classification, which makes the assumption that each example is assigned\n",
    "to one and only one label.\n",
    "For illustration purposes, we use the Iris flower data set, which consists of a classification into three\n",
    "mutually-exclusive classes. Let us assume these classes are $A$, $B$, and $C$.\n",
    "\n",
    "While one could train three unary\n",
    "predicates $A(x)$, $B(x)$ and $C(x)$, it turns out to be more effective if this problem is modelled by a single\n",
    "binary predicate $P(x,l)$, where $l$ is a variable denoting a multi-class label, in this case classes $A$, $B$, or $C$.\n",
    "This syntax allows one to write statements quantifying over the classes, e.g. $\\forall x(\\exists l(P(x, l)))$, which states\n",
    "that each example should be assigned at least one class.\n",
    "\n",
    "Note that since the classes are mutually-exclusive in this case, the output layer of the $MLP$ representing predicate $P(x,l)$\n",
    "will be a softmax layer, instead of a sigmoid function, to learn the probability of $A$, $B$, and $C$.\n",
    "\n",
    "For this specific task, LTN uses the following language and grounding:\n",
    "\n",
    "**Domains:**\n",
    "- $items$, denoting the examples from the Iris flower data set;\n",
    "- $labels$, denoting the class labels.\n",
    "\n",
    "**Variables:**\n",
    "- $x_A, x_B, x_C$ for the positive examples of classes $A$, $B$, and $C$;\n",
    "- $x$ for all examples;\n",
    "- $D(x_A) = D(x_B) = D(x_C) = D(x) = items$.\n",
    "\n",
    "**Constants:**\n",
    "- $l_A, l_B, l_C$, the labels of classes $A$ (Iris setosa), $B$ (Iris virginica), $C$ (Iris versicolor), respectively;\n",
    "- $D(l_A) = D(l_B) = D(l_C) = labels$.\n",
    "\n",
    "**Predicates:**\n",
    "- $P(x,l)$ denoting the fact that item $x$ is classified as $l$;\n",
    "- $D_{in}(P) = items,labels$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "- $\\forall x_A P(x_A, l_A)$: all the examples of class $A$ should have label $l_A$;\n",
    "- $\\forall x_B P(x_B, l_B)$: all the examples of class $B$ should have label $l_B$;\n",
    "- $\\forall x_C P(x_C, l_C)$: all the examples of class $C$ should have label $l_C$.\n",
    "\n",
    "Notice that rules about exclusiveness such as $\\forall x (P(x, l_A) \\implies (\\lnot P(x, l_B) \\land \\lnot P(x, l_C)))$ are not included since such constraints are already imposed by the\n",
    "grounding of $P$ below, more specifically by the *softmax* function.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{4}$, items are described by 4 features: the length and the width of the sepals and\n",
    "petals, in centimeters;\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{3}$, we use a one-hot encoding to represent classes;\n",
    "- $\\mathcal{G}(x_A) \\in \\mathbb{R}^{m_1 \\times 4}$, that is, $\\mathcal{G}(x_A)$ is a sequence of $m_1$ examples of class $A$;\n",
    "- $\\mathcal{G}(x_B) \\in \\mathbb{R}^{m_2 \\times 4}$, that is, $\\mathcal{G}(x_B)$ is a sequence of $m_2$ examples of class $B$;\n",
    "- $\\mathcal{G}(x_C) \\in \\mathbb{R}^{m_3 \\times 4}$, that is, $\\mathcal{G}(x_C)$ is a sequence of $m_3$ examples of class $C$;\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{(m_1+m_2+m_3) \\times 4}$, that is, $\\mathcal{G}(x)$ is a sequence of all the examples;\n",
    "- $\\mathcal{G}(l_A) = [1, 0, 0]$, $\\mathcal{G}(l_B) = [0, 1, 0]$, $\\mathcal{G}(l_C) = [0, 0, 1]$;\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\operatorname{softmax}\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$, where $MLP$\n",
    "has three output neurons corresponding to as many classes, and $\\cdot$ denotes the dot product as a way of selecting an\n",
    "output for $\\mathcal{G}(P \\mid \\theta)$. In fact, multiplying the $MLP$â€™s output by the one-hot vector $l^\\top$ gives the probability\n",
    "corresponding to the class denoted by $l$.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import the dataset.\n",
    "\n",
    "The Iris flower dataset has three classes with 50 examples each. Every example is represented by 4 features. The dataset\n",
    "is already subdivided into train and test set."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"datasets/iris_training.csv\")\n",
    "test_data = pd.read_csv(\"datasets/iris_test.csv\")\n",
    "\n",
    "train_labels = train_data.pop(\"species\")\n",
    "test_labels = test_data.pop(\"species\")\n",
    "\n",
    "train_data = torch.tensor(train_data.to_numpy()).float()\n",
    "test_data = torch.tensor(test_data.to_numpy()).float()\n",
    "train_labels = torch.tensor(train_labels.to_numpy()).long()\n",
    "test_labels = torch.tensor(test_labels.to_numpy()).long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $P$, constants $l_A$, $l_B$, $l_C$,\n",
    "universal quantifier, and the `SatAgg` operator.\n",
    "\n",
    "For the quantifier, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $P$, we have two models. The first one implements an $MLP$ which outputs the logits for the three classes of\n",
    "the Iris flower dataset, given an example $x$ in input. The second model takes as input a labelled example $(x,l)$, it computes the logits\n",
    "using the first model and then returns the prediction (*softmax*) for class $l$.\n",
    "\n",
    "We need two separated models because we need both logits and probabilities. Logits are used to compute the classification\n",
    "accuracy, while probabilities are interpreted as truth values to compute the satisfaction level of the knowledge base.\n",
    "\n",
    "The constants $l_A$, $l_B$, and $l_C$, represent the one-hot labels for the three classes, as we have already seen in the\n",
    "definition of the grounding for this task.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import ltn\n",
    "\n",
    "# we define the constants\n",
    "l_A = ltn.Constant(torch.tensor([1, 0, 0]))\n",
    "l_B = ltn.Constant(torch.tensor([0, 1, 0]))\n",
    "l_C = ltn.Constant(torch.tensor([0, 0, 1]))\n",
    "\n",
    "# we define predicate P\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logits for the classes given an input example. It does not compute the softmax, so the output\n",
    "    are not normalized.\n",
    "    This is done to separate the accuracy computation from the satisfaction level computation. Go through the example\n",
    "    to understand it.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes=(4, 16, 16, 8, 3)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Method which defines the forward phase of the neural network for our multi class classification task.\n",
    "        In particular, it returns the logits for the classes given an input example.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :param training: whether the network is in training mode (dropout applied) or validation mode (dropout not applied)\n",
    "        :return: logits for example x\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.softmax(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "mlp = MLP()\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data.\n",
    "\n",
    "Then, we define functions to evaluate the model performances. The model is evaluated on the test set using the following metrics:\n",
    "- the satisfaction level of the knowledge base: measure the ability of LTN to satisfy the knowledge;\n",
    "- the classification accuracy: measure the quality of the predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# this is a standard PyTorch DataLoader to load the dataset for the training and testing of the model\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels = self.labels[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test)\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels in loader:\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0])\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1])\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A)),\n",
    "            Forall(x_B, P(x_B, l_B)),\n",
    "            Forall(x_C, P(x_C, l_C))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "# it computes the overall accuracy of the predictions of the trained model using the given data loader\n",
    "# (train or test)\n",
    "def compute_accuracy(loader):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels in loader:\n",
    "        predictions = mlp(data).detach().numpy()\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        mean_accuracy += accuracy_score(labels, predictions)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n",
    "\n",
    "# create train and test loader\n",
    "train_loader = DataLoader(train_data, train_labels, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, test_labels, 64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function with $\\mathcal{K}=\\{\\forall x_A P(x_A, l_A),\\forall x_B P(x_B, l_B),\\forall x_C P(x_C, l_C)\\}$\n",
    "is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "In the following, we learn our LTN in the multi-class single-label classification task using the satisfaction of the knowledge base as\n",
    "an objective. In other words, we want to learn the parameters $\\theta$ of binary predicate $P$ in such a way the three\n",
    "axioms in the knowledge base are maximally satisfied. We train our model for 500 epochs and use the `Adam` optimizer.\n",
    "\n",
    "The following figure shows the LTN computational graph for this specific task.\n",
    "\n",
    "![Computational graph](/examples/images/multi-class-single-label-classification.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.6707 | Train Sat 0.330 | Test Sat 0.331 | Train Acc 0.299 | Test Acc 0.467\n",
      " epoch 20 | loss 0.6257 | Train Sat 0.400 | Test Sat 0.401 | Train Acc 0.661 | Test Acc 0.500\n",
      " epoch 40 | loss 0.5036 | Train Sat 0.535 | Test Sat 0.536 | Train Acc 0.702 | Test Acc 0.533\n",
      " epoch 60 | loss 0.4245 | Train Sat 0.605 | Test Sat 0.605 | Train Acc 0.725 | Test Acc 0.567\n",
      " epoch 80 | loss 0.3847 | Train Sat 0.660 | Test Sat 0.664 | Train Acc 0.958 | Test Acc 0.967\n",
      " epoch 100 | loss 0.3258 | Train Sat 0.734 | Test Sat 0.744 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 120 | loss 0.2724 | Train Sat 0.796 | Test Sat 0.807 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 140 | loss 0.2134 | Train Sat 0.831 | Test Sat 0.836 | Train Acc 0.984 | Test Acc 0.967\n",
      " epoch 160 | loss 0.2327 | Train Sat 0.841 | Test Sat 0.846 | Train Acc 0.975 | Test Acc 0.967\n",
      " epoch 180 | loss 0.1894 | Train Sat 0.862 | Test Sat 0.850 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 200 | loss 0.1727 | Train Sat 0.869 | Test Sat 0.851 | Train Acc 0.967 | Test Acc 0.967\n",
      " epoch 220 | loss 0.1857 | Train Sat 0.855 | Test Sat 0.855 | Train Acc 0.974 | Test Acc 0.967\n",
      " epoch 240 | loss 0.2109 | Train Sat 0.871 | Test Sat 0.856 | Train Acc 0.974 | Test Acc 0.967\n",
      " epoch 260 | loss 0.1404 | Train Sat 0.883 | Test Sat 0.851 | Train Acc 0.983 | Test Acc 0.967\n",
      " epoch 280 | loss 0.1603 | Train Sat 0.873 | Test Sat 0.839 | Train Acc 0.984 | Test Acc 0.933\n",
      " epoch 300 | loss 0.1709 | Train Sat 0.876 | Test Sat 0.854 | Train Acc 0.974 | Test Acc 0.967\n",
      " epoch 320 | loss 0.1731 | Train Sat 0.862 | Test Sat 0.856 | Train Acc 0.973 | Test Acc 0.967\n",
      " epoch 340 | loss 0.1245 | Train Sat 0.891 | Test Sat 0.854 | Train Acc 0.975 | Test Acc 0.967\n",
      " epoch 360 | loss 0.1437 | Train Sat 0.884 | Test Sat 0.849 | Train Acc 0.991 | Test Acc 0.933\n",
      " epoch 380 | loss 0.1328 | Train Sat 0.875 | Test Sat 0.851 | Train Acc 0.983 | Test Acc 0.933\n",
      " epoch 400 | loss 0.1742 | Train Sat 0.860 | Test Sat 0.860 | Train Acc 0.974 | Test Acc 0.967\n",
      " epoch 420 | loss 0.1593 | Train Sat 0.879 | Test Sat 0.853 | Train Acc 0.974 | Test Acc 0.933\n",
      " epoch 440 | loss 0.1286 | Train Sat 0.899 | Test Sat 0.845 | Train Acc 0.992 | Test Acc 0.933\n",
      " epoch 460 | loss 0.1391 | Train Sat 0.882 | Test Sat 0.856 | Train Acc 0.973 | Test Acc 0.967\n",
      " epoch 480 | loss 0.1554 | Train Sat 0.886 | Test Sat 0.846 | Train Acc 0.991 | Test Acc 0.933\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x_A = ltn.Variable(\"x_A\", data[labels == 0]) # class A examples\n",
    "        x_B = ltn.Variable(\"x_B\", data[labels == 1]) # class B examples\n",
    "        x_C = ltn.Variable(\"x_C\", data[labels == 2]) # class C examples\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_A, P(x_A, l_A, training=True)),\n",
    "            Forall(x_B, P(x_B, l_B, training=True)),\n",
    "            Forall(x_C, P(x_C, l_C, training=True))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f\"\n",
    "              %(epoch, train_loss, compute_sat_level(train_loader), compute_sat_level(test_loader),\n",
    "                    compute_accuracy(train_loader), compute_accuracy(test_loader)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that variables $x_A$, $x_B$, and $x_C$ are grounded batch by batch with new data arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg` takes as input the three axioms and returns one truth value which can be interpreted as the satisfaction\n",
    "level of the knowledge base.\n",
    "\n",
    "Note that after 80 epochs the test accuracy is around 1. This shows the power of LTN in learning\n",
    "the multi-class single-label classification task only using the satisfaction of a knowledge base as an objective."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}