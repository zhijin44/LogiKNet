{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-class multi-label classification\n",
    "\n",
    "We now turn to multi-label classification, whereby multiple labels can be assigned to each example. As a first example\n",
    "of the reach of LTNs, we shall see how the previous example can be extended naturally using LTN to account for multiple\n",
    "labels, which is not always a trivial extension for most ML algorithms.\n",
    "\n",
    "The standard approach to the multi-label problem is to provide explicit negative examples for each class. By contrast,\n",
    "LTN can use background knowledge to relate classes directly to each other, thus becoming a powerful tool in the case of\n",
    "the multi-label problem, where typically the labelled data is scarce.\n",
    "\n",
    "We explore the Leptograpsus crabs dataset, consisting of 200 examples of 5 morphological measurements of 50 crabs.\n",
    "The task is to classify the crabs according to their colour and sex. There are four labels: blue, orange, male, and female.\n",
    "The colour labels are mutually-exclusive, and so are the labels for sex. LTN will be used to specify such information\n",
    "logically.\n",
    "\n",
    "For this specific task, LTN uses the following language and grounding:\n",
    "\n",
    "**Domains:**\n",
    "- $items$, denoting the examples from the crabs data set;\n",
    "- $labels$, denoting the class labels.\n",
    "\n",
    "**Variables:**\n",
    "- $x_{blue}, x_{orange}, x_{male}, x_{female}$ for the positive examples of each class;\n",
    "- $x$, used to denote all the examples;\n",
    "- $D(x_{blue}) = D(x_{orange}) = D(x_{male}) = D(x_{female}) = D(x) = items$.\n",
    "\n",
    "**Constants:**\n",
    "- $l_{blue}, l_{orange}, l_{male}, l_{female}$: the labels of each class;\n",
    "- $D(l_{blue}) = D(l_{orange}) = D(l_{male}) = D(l_{female}) = labels$.\n",
    "\n",
    "**Predicates:**\n",
    "- $P(x,l)$ denoting the fact that item $x$ is labelled as $l$;\n",
    "- $D_{in}(P) = items,labels$.\n",
    "\n",
    "**Axioms:**\n",
    "\n",
    "- $\\forall x_{blue} P(x_{blue}, l_{blue})$: all the examples coloured by blue should have label $l_{blue}$;\n",
    "- $\\forall x_{orange} P(x_{orange}, l_{orange})$: all the examples coloured by orange should have label $l_{orange}$;\n",
    "- $\\forall x_{male} P(x_{male}, l_{male})$: all the examples that are males should have label $l_{male}$;\n",
    "- $\\forall x_{female} P(x_{female}, l_{female})$: all the examples that are females should have label $l_{female}$;\n",
    "- $\\forall x \\lnot (P(x, l_{blue}) \\land P(x, l_{orange}))$: if an example $x$ is labelled as blue, it cannot be labelled\n",
    "as orange too;\n",
    "- $\\forall x \\lnot (P(x, l_{male}) \\land P(x, l_{female}))$: if an example $x$ is labelled as male, it cannot be labelled\n",
    "as female too.\n",
    "\n",
    "Notice how the last two logical rules represent the mutual exclusion of the labels on colour and sex, respectively.\n",
    "As a result, negative examples are not used explicitly in this specification.\n",
    "\n",
    "\n",
    "**Grounding:**\n",
    "- $\\mathcal{G}(items)=\\mathbb{R}^{5}$, items are described by 5 features;\n",
    "- $\\mathcal{G}(labels)=\\mathbb{N}^{4}$, we use a one-hot encoding to represent labels;\n",
    "- $\\mathcal{G}(x_{blue}) \\in \\mathbb{R}^{m_1 \\times 5}, \\mathcal{G}(x_{orange}) \\in \\mathbb{R}^{m_2 \\times 5},\\mathcal{G}(x_{male}) \\in \\mathbb{R}^{m_3 \\times 5},\\mathcal{G}(x_{female}) \\in \\mathbb{R}^{m_4 \\times 5}$.\n",
    "These sequences are not mutually-exclusive, one example can for instance be in both $x_{blue}$ and $x_{male}$;\n",
    "- $\\mathcal{G}(x) \\in \\mathbb{R}^{m \\times 5}$, that is, $\\mathcal{G}(x)$ is a sequence of all the examples;\n",
    "- $\\mathcal{G}(l_{blue}) = [1, 0, 0, 0]$, $\\mathcal{G}(l_{orange}) = [0, 1, 0, 0]$, $\\mathcal{G}(l_{male}) = [0, 0, 1, 0]$, $\\mathcal{G}(l_{female}) = [0, 0, 0, 1]$;\n",
    "- $\\mathcal{G}(P \\mid \\theta): x,l \\mapsto l^\\top \\cdot \\sigma\\left(\\operatorname{MLP}_{\\theta}(x)\\right)$, where $MLP$\n",
    "has four output neurons corresponding to as many labels, and $\\cdot$ denotes the dot product as a way of selecting an\n",
    "output for $\\mathcal{G}(P \\mid \\theta)$. In fact, multiplying the $MLP$â€™s output by the one-hot vector $l^\\top$ gives the probability\n",
    "corresponding to the label denoted by $l$. By contrast with the previous example, notice the use\n",
    "of a *sigmoid* function instead of a *softmax* function. We need that because labels are not mutually exclusive anymore.\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Now, let's import the dataset.\n",
    "\n",
    "The Leptograpsus crabs dataset consists of 200 examples. Every example is represented by 5 features. The dataset\n",
    "is subdivided into train and test set. In particular, we use 160 examples for training and 40 for test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/crabs.dat\", sep=\" \", skipinitialspace=True)\n",
    "df = df.sample(frac=1)  # shuffle dataset\n",
    "df = df.replace({'B': 0, 'O': 1, 'M': 2, 'F': 3})\n",
    "\n",
    "features = torch.tensor(df[['FL', 'RW', 'CL', 'CW', 'BD']].to_numpy())\n",
    "labels_sex = torch.tensor(df['sex'].to_numpy())\n",
    "labels_color = torch.tensor(df['sp'].to_numpy())\n",
    "\n",
    "train_data = features[:160].float()\n",
    "test_data = features[160:].float()\n",
    "train_sex_labels = labels_sex[:160].long()\n",
    "test_sex_labels = labels_sex[160:].long()\n",
    "train_color_labels = labels_color[:160].long()\n",
    "test_color_labels = labels_color[160:].long()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LTN setting\n",
    "\n",
    "In order to define our knowledge base (axioms), we need to define predicate $P$, constants $l_{blue}$, $l_{orange}$, $l_{male}$,\n",
    "$l_{female}$, connectives, universal quantifier, and the `SatAgg` operator.\n",
    "\n",
    "For the connectives and quantifier, we use the stable product configuration (seen in the tutorials).\n",
    "\n",
    "For predicate $P$, we have two models. The first one implements an $MLP$ which outputs the logits for the four classes of\n",
    "the dataset, given an example $x$ in input. The second model takes as input a labelled example $(x,l)$, it computes the logits\n",
    "using the first model and then returns the prediction (*sigmoid*) for class $l$.\n",
    "\n",
    "The constants $l_{blue}$, $l_{orange}$, $l_{male}$, and $l_{female}$, represent the one-hot labels for the four classes, as we have already seen in the\n",
    "definition of the grounding for this task.\n",
    "\n",
    "`SatAgg` is defined using the `pMeanError` aggregator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import ltn\n",
    "\n",
    "# we define the constants\n",
    "l_blue = ltn.Constant(torch.tensor([1, 0, 0, 0]))\n",
    "l_orange = ltn.Constant(torch.tensor([0, 1, 0, 0]))\n",
    "l_male = ltn.Constant(torch.tensor([0, 0, 1, 0]))\n",
    "l_female = ltn.Constant(torch.tensor([0, 0, 0, 1]))\n",
    "\n",
    "# we define predicate P\n",
    "class MLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model returns the logits for the classes given an input example. It does not compute the softmax, so the output\n",
    "    are not normalized.\n",
    "    This is done to separate the accuracy computation from the satisfaction level computation. Go through the example\n",
    "    to understand it.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes=(5, 16, 16, 8, 4)):\n",
    "        super(MLP, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.dropout = torch.nn.Dropout(0.2)\n",
    "        self.linear_layers = torch.nn.ModuleList([torch.nn.Linear(layer_sizes[i - 1], layer_sizes[i])\n",
    "                                                  for i in range(1, len(layer_sizes))])\n",
    "\n",
    "    def forward(self, x, training=False):\n",
    "        \"\"\"\n",
    "        Method which defines the forward phase of the neural network for our multi class classification task.\n",
    "        In particular, it returns the logits for the classes given an input example.\n",
    "\n",
    "        :param x: the features of the example\n",
    "        :param training: whether the network is in training mode (dropout applied) or validation mode (dropout not applied)\n",
    "        :return: logits for example x\n",
    "        \"\"\"\n",
    "        for layer in self.linear_layers[:-1]:\n",
    "            x = self.elu(layer(x))\n",
    "            if training:\n",
    "                x = self.dropout(x)\n",
    "        logits = self.linear_layers[-1](x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class LogitsToPredicate(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This model has inside a logits model, that is a model which compute logits for the classes given an input example x.\n",
    "    The idea of this model is to keep logits and probabilities separated. The logits model returns the logits for an example,\n",
    "    while this model returns the probabilities given the logits model.\n",
    "\n",
    "    In particular, it takes as input an example x and a class label l. It applies the logits model to x to get the logits.\n",
    "    Then, it applies a softmax function to get the probabilities per classes. Finally, it returns only the probability related\n",
    "    to the given class l.\n",
    "    \"\"\"\n",
    "    def __init__(self, logits_model):\n",
    "        super(LogitsToPredicate, self).__init__()\n",
    "        self.logits_model = logits_model\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, l, training=False):\n",
    "        logits = self.logits_model(x, training=training)\n",
    "        probs = self.sigmoid(logits)\n",
    "        out = torch.sum(probs * l, dim=1)\n",
    "        return out\n",
    "\n",
    "mlp = MLP()\n",
    "P = ltn.Predicate(LogitsToPredicate(mlp))\n",
    "\n",
    "# we define the connectives, quantifiers, and the SatAgg\n",
    "Not = ltn.Connective(ltn.fuzzy_ops.NotStandard())\n",
    "And = ltn.Connective(ltn.fuzzy_ops.AndProd())\n",
    "Forall = ltn.Quantifier(ltn.fuzzy_ops.AggregPMeanError(p=2), quantifier=\"f\")\n",
    "SatAgg = ltn.fuzzy_ops.SatAgg()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Utils\n",
    "\n",
    "Now, we need to define some utility classes and functions.\n",
    "\n",
    "We define a standard PyTorch data loader, which takes as input the dataset and returns a generator of batches of data.\n",
    "In particular, we need a data loader instance for training data and one for testing data.\n",
    "\n",
    "Then, we define functions to evaluate the model performances. The model is evaluated on the test set using the following metrics:\n",
    "- the satisfaction level of the knowledge base: measure the ability of LTN to satisfy the knowledge;\n",
    "- the classification accuracy: this time, the accuracy is defined as $1 - HL$, where $HL$ is the average Hamming loss,\n",
    "i.e. the fraction of labels predicted incorrectly, with a classification threshold of 0.5 (given an example $u$,\n",
    "if the model outputs a value greater than 0.5 for class $C$ then $u$ is deemed as belonging to class $C$)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self,\n",
    "                 data,\n",
    "                 labels,\n",
    "                 batch_size=1,\n",
    "                 shuffle=True):\n",
    "        self.data = data\n",
    "        self.labels_sex = labels[0]\n",
    "        self.labels_color = labels[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data.shape[0] / self.batch_size))\n",
    "\n",
    "    def __iter__(self):\n",
    "        n = self.data.shape[0]\n",
    "        idxlist = list(range(n))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxlist)\n",
    "\n",
    "        for _, start_idx in enumerate(range(0, n, self.batch_size)):\n",
    "            end_idx = min(start_idx + self.batch_size, n)\n",
    "            data = self.data[idxlist[start_idx:end_idx]]\n",
    "            labels_sex = self.labels_sex[idxlist[start_idx:end_idx]]\n",
    "            labels_color = self.labels_color[idxlist[start_idx:end_idx]]\n",
    "\n",
    "            yield data, labels_sex, labels_color\n",
    "\n",
    "\n",
    "# define metrics for evaluation of the model\n",
    "\n",
    "# it computes the overall satisfaction level on the knowledge base using the given data loader (train or test)\n",
    "def compute_sat_level(loader):\n",
    "    mean_sat = 0\n",
    "    for data, labels_sex, labels_color in loader:\n",
    "        x = ltn.Variable(\"x\", data)\n",
    "        x_blue = ltn.Variable(\"x_blue\", data[labels_color == 0])\n",
    "        x_orange = ltn.Variable(\"x_orange\", data[labels_color == 1])\n",
    "        x_male = ltn.Variable(\"x_male\", data[labels_sex == 2])\n",
    "        x_female = ltn.Variable(\"x_female\", data[labels_sex == 3])\n",
    "        mean_sat += SatAgg(\n",
    "            Forall(x_blue, P(x_blue, l_blue)),\n",
    "            Forall(x_orange, P(x_orange, l_orange)),\n",
    "            Forall(x_male, P(x_male, l_male)),\n",
    "            Forall(x_female, P(x_female, l_female)),\n",
    "            Forall(x, Not(And(P(x, l_blue), P(x, l_orange)))),\n",
    "            Forall(x, Not(And(P(x, l_male), P(x, l_female))))\n",
    "        )\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat\n",
    "\n",
    "# it computes the overall accuracy of the predictions of the trained model using the given data loader\n",
    "# (train or test)\n",
    "def compute_accuracy(loader, threshold=0.5):\n",
    "    mean_accuracy = 0.0\n",
    "    for data, labels_sex, labels_color in loader:\n",
    "        predictions = mlp(data).detach().numpy()\n",
    "        labels_male = (labels_sex == 2)\n",
    "        labels_female = (labels_sex == 3)\n",
    "        labels_blue = (labels_color == 0)\n",
    "        labels_orange = (labels_color == 1)\n",
    "        onehot = np.stack([labels_blue, labels_orange, labels_male, labels_female], axis=-1).astype(np.int32)\n",
    "        predictions = predictions > threshold\n",
    "        predictions = predictions.astype(np.int32)\n",
    "        nonzero = np.count_nonzero(onehot - predictions, axis=-1).astype(np.float32)\n",
    "        multilabel_hamming_loss = nonzero / predictions.shape[-1]\n",
    "        mean_accuracy += np.mean(1 - multilabel_hamming_loss)\n",
    "\n",
    "    return mean_accuracy / len(loader)\n",
    "\n",
    "# create train and test loader\n",
    "train_loader = DataLoader(train_data, (train_sex_labels, train_color_labels), 64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, (test_sex_labels, test_color_labels), 64, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning\n",
    "\n",
    "Let us define $D$ the data set of all examples. The objective function is given by $\\operatorname{SatAgg}_{\\phi \\in \\mathcal{K}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{D}}(\\phi)$.\n",
    "\n",
    "In practice, the optimizer uses the following loss function:\n",
    "\n",
    "$\\boldsymbol{L}=\\left(1-\\underset{\\phi \\in \\mathcal{K}}{\\operatorname{SatAgg}} \\mathcal{G}_{\\boldsymbol{\\theta}, x \\leftarrow \\boldsymbol{B}}(\\phi)\\right)$\n",
    "\n",
    "where $B$ is a mini batch sampled from $D$.\n",
    "\n",
    "### Querying\n",
    "\n",
    "To illustrate the learning of constraints by LTN, we have queried three formulas\n",
    "that were not explicitly part of the knowledge base, over time during learning:\n",
    "- $\\phi_1: \\forall x (P(x, l_{blue}) \\implies \\lnot P(x, l_{orange}))$;\n",
    "- $\\phi_2: \\forall x (P(x, l_{blue}) \\implies P(x, l_{orange}))$;\n",
    "- $\\phi_3: \\forall x (P(x, l_{blue}) \\implies P(x, l_{male}))$.\n",
    "\n",
    "For querying, we use $p=5$ when approximating the universal quantifiers with\n",
    "`pMeanError`. A higher $p$ denotes a stricter universal quantification with a stronger\n",
    "focus on outliers. We should expect $\\phi_1$ to hold true (every\n",
    "blue crab cannot be orange and vice-versa), and we should expect $\\phi_2$ (every blue crab is also orange) and $\\phi_3$\n",
    "(every blue crab is male) to be false.\n",
    "\n",
    "In the following, we define some functions computing the three formulas and the implication connective, since we need it\n",
    "to define the formulas."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "Implies = ltn.Connective(ltn.fuzzy_ops.ImpliesReichenbach())\n",
    "\n",
    "def phi1(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), Not(P(x, l_orange))), p=5)\n",
    "\n",
    "def phi2(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), P(x, l_orange)), p=5)\n",
    "\n",
    "def phi3(features):\n",
    "    x = ltn.Variable(\"x\", features)\n",
    "    return Forall(x, Implies(P(x, l_blue), P(x, l_male)), p=5)\n",
    "\n",
    "# it computes the satisfaction level of a formula phi using the given data loader (train or test)\n",
    "def compute_sat_level_phi(loader, phi):\n",
    "    mean_sat = 0\n",
    "    for features, _, _ in loader:\n",
    "        mean_sat += phi(features).value\n",
    "    mean_sat /= len(loader)\n",
    "    return mean_sat"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we learn our LTN in the multi-class multi-label classification task using the satisfaction of the knowledge base as\n",
    "an objective. In other words, we want to learn the parameters $\\theta$ of binary predicate $P$ in such a way the three\n",
    "axioms in the knowledge base are maximally satisfied. We train our model for 500 epochs and use the `Adam` optimizer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 0 | loss 0.5147 | Train Sat 0.502 | Test Sat 0.510 | Train Acc 0.497 | Test Acc 0.500 | Test Sat Phi 1 0.813 | Test Sat Phi 2 0.882 | Test Sat Phi 3 0.838 \n",
      " epoch 20 | loss 0.3738 | Train Sat 0.626 | Test Sat 0.626 | Train Acc 0.531 | Test Acc 0.475 | Test Sat Phi 1 0.554 | Test Sat Phi 2 0.794 | Test Sat Phi 3 0.752 \n",
      " epoch 40 | loss 0.3693 | Train Sat 0.631 | Test Sat 0.630 | Train Acc 0.500 | Test Acc 0.500 | Test Sat Phi 1 0.554 | Test Sat Phi 2 0.787 | Test Sat Phi 3 0.782 \n",
      " epoch 60 | loss 0.3562 | Train Sat 0.644 | Test Sat 0.641 | Train Acc 0.770 | Test Acc 0.725 | Test Sat Phi 1 0.578 | Test Sat Phi 2 0.742 | Test Sat Phi 3 0.809 \n",
      " epoch 80 | loss 0.3108 | Train Sat 0.689 | Test Sat 0.679 | Train Acc 0.812 | Test Acc 0.800 | Test Sat Phi 1 0.653 | Test Sat Phi 2 0.594 | Test Sat Phi 3 0.804 \n",
      " epoch 100 | loss 0.2584 | Train Sat 0.740 | Test Sat 0.718 | Train Acc 0.861 | Test Acc 0.819 | Test Sat Phi 1 0.741 | Test Sat Phi 2 0.397 | Test Sat Phi 3 0.784 \n",
      " epoch 120 | loss 0.2273 | Train Sat 0.774 | Test Sat 0.744 | Train Acc 0.880 | Test Acc 0.837 | Test Sat Phi 1 0.780 | Test Sat Phi 2 0.274 | Test Sat Phi 3 0.740 \n",
      " epoch 140 | loss 0.2002 | Train Sat 0.801 | Test Sat 0.770 | Train Acc 0.939 | Test Acc 0.887 | Test Sat Phi 1 0.798 | Test Sat Phi 2 0.258 | Test Sat Phi 3 0.611 \n",
      " epoch 160 | loss 0.1667 | Train Sat 0.837 | Test Sat 0.795 | Train Acc 0.954 | Test Acc 0.894 | Test Sat Phi 1 0.817 | Test Sat Phi 2 0.240 | Test Sat Phi 3 0.505 \n",
      " epoch 180 | loss 0.1425 | Train Sat 0.860 | Test Sat 0.813 | Train Acc 0.964 | Test Acc 0.913 | Test Sat Phi 1 0.851 | Test Sat Phi 2 0.191 | Test Sat Phi 3 0.421 \n",
      " epoch 200 | loss 0.1315 | Train Sat 0.872 | Test Sat 0.819 | Train Acc 0.971 | Test Acc 0.906 | Test Sat Phi 1 0.854 | Test Sat Phi 2 0.173 | Test Sat Phi 3 0.350 \n",
      " epoch 220 | loss 0.1134 | Train Sat 0.889 | Test Sat 0.829 | Train Acc 0.975 | Test Acc 0.919 | Test Sat Phi 1 0.873 | Test Sat Phi 2 0.170 | Test Sat Phi 3 0.319 \n",
      " epoch 240 | loss 0.1127 | Train Sat 0.894 | Test Sat 0.842 | Train Acc 0.970 | Test Acc 0.931 | Test Sat Phi 1 0.874 | Test Sat Phi 2 0.170 | Test Sat Phi 3 0.314 \n",
      " epoch 260 | loss 0.1027 | Train Sat 0.901 | Test Sat 0.848 | Train Acc 0.991 | Test Acc 0.931 | Test Sat Phi 1 0.878 | Test Sat Phi 2 0.162 | Test Sat Phi 3 0.303 \n",
      " epoch 280 | loss 0.1085 | Train Sat 0.902 | Test Sat 0.843 | Train Acc 0.977 | Test Acc 0.931 | Test Sat Phi 1 0.875 | Test Sat Phi 2 0.152 | Test Sat Phi 3 0.277 \n",
      " epoch 300 | loss 0.0925 | Train Sat 0.910 | Test Sat 0.852 | Train Acc 0.995 | Test Acc 0.919 | Test Sat Phi 1 0.875 | Test Sat Phi 2 0.153 | Test Sat Phi 3 0.281 \n",
      " epoch 320 | loss 0.0913 | Train Sat 0.912 | Test Sat 0.848 | Train Acc 0.984 | Test Acc 0.931 | Test Sat Phi 1 0.873 | Test Sat Phi 2 0.149 | Test Sat Phi 3 0.265 \n",
      " epoch 340 | loss 0.0875 | Train Sat 0.919 | Test Sat 0.856 | Train Acc 0.996 | Test Acc 0.919 | Test Sat Phi 1 0.876 | Test Sat Phi 2 0.147 | Test Sat Phi 3 0.273 \n",
      " epoch 360 | loss 0.0925 | Train Sat 0.919 | Test Sat 0.854 | Train Acc 0.993 | Test Acc 0.919 | Test Sat Phi 1 0.875 | Test Sat Phi 2 0.144 | Test Sat Phi 3 0.264 \n",
      " epoch 380 | loss 0.0844 | Train Sat 0.917 | Test Sat 0.845 | Train Acc 0.984 | Test Acc 0.938 | Test Sat Phi 1 0.869 | Test Sat Phi 2 0.145 | Test Sat Phi 3 0.247 \n",
      " epoch 400 | loss 0.0777 | Train Sat 0.919 | Test Sat 0.858 | Train Acc 0.991 | Test Acc 0.938 | Test Sat Phi 1 0.909 | Test Sat Phi 2 0.156 | Test Sat Phi 3 0.269 \n",
      " epoch 420 | loss 0.0882 | Train Sat 0.925 | Test Sat 0.853 | Train Acc 0.995 | Test Acc 0.950 | Test Sat Phi 1 0.869 | Test Sat Phi 2 0.141 | Test Sat Phi 3 0.259 \n",
      " epoch 440 | loss 0.0711 | Train Sat 0.930 | Test Sat 0.852 | Train Acc 0.990 | Test Acc 0.950 | Test Sat Phi 1 0.866 | Test Sat Phi 2 0.138 | Test Sat Phi 3 0.255 \n",
      " epoch 460 | loss 0.0799 | Train Sat 0.928 | Test Sat 0.854 | Train Acc 0.991 | Test Acc 0.944 | Test Sat Phi 1 0.875 | Test Sat Phi 2 0.137 | Test Sat Phi 3 0.256 \n",
      " epoch 480 | loss 0.0731 | Train Sat 0.945 | Test Sat 0.851 | Train Acc 0.995 | Test Acc 0.938 | Test Sat Phi 1 0.868 | Test Sat Phi 2 0.137 | Test Sat Phi 3 0.247 \n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(P.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (data, labels_sex, labels_color) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        # we ground the variables with current batch data\n",
    "        x = ltn.Variable(\"x\", data)\n",
    "        x_blue = ltn.Variable(\"x_blue\", data[labels_color == 0])\n",
    "        x_orange = ltn.Variable(\"x_orange\", data[labels_color == 1])\n",
    "        x_male = ltn.Variable(\"x_male\", data[labels_sex == 2])\n",
    "        x_female = ltn.Variable(\"x_female\", data[labels_sex == 3])\n",
    "        sat_agg = SatAgg(\n",
    "            Forall(x_blue, P(x_blue, l_blue)),\n",
    "            Forall(x_orange, P(x_orange, l_orange)),\n",
    "            Forall(x_male, P(x_male, l_male)),\n",
    "            Forall(x_female, P(x_female, l_female)),\n",
    "            Forall(x, Not(And(P(x, l_blue), P(x, l_orange)))),\n",
    "            Forall(x, Not(And(P(x, l_male), P(x, l_female))))\n",
    "        )\n",
    "        loss = 1. - sat_agg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss = train_loss / len(train_loader)\n",
    "\n",
    "    # we print metrics every 20 epochs of training\n",
    "    if epoch % 20 == 0:\n",
    "        print(\" epoch %d | loss %.4f | Train Sat %.3f | Test Sat %.3f | Train Acc %.3f | Test Acc %.3f | \"\n",
    "                        \"Test Sat Phi 1 %.3f | Test Sat Phi 2 %.3f | Test Sat Phi 3 %.3f \" %\n",
    "              (epoch, train_loss, compute_sat_level(train_loader),\n",
    "                        compute_sat_level(test_loader),\n",
    "                        compute_accuracy(train_loader), compute_accuracy(test_loader),\n",
    "                        compute_sat_level_phi(test_loader, phi1), compute_sat_level_phi(test_loader, phi2),\n",
    "                        compute_sat_level_phi(test_loader, phi3)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that variables $x_{blue}$, $x_{orange}$, $x_{male}$, and $x_{female}$ are grounded batch by batch with new data\n",
    "arriving from the data loader. This is exactly what\n",
    "we mean with $\\mathcal{G}_{x \\leftarrow \\boldsymbol{B}}(\\phi(x))$, where $B$ is a mini-batch sampled by the data loader.\n",
    "\n",
    "Notice also that `SatAgg` takes as input the four axioms and returns one truth value which can be interpreted as the satisfaction\n",
    "level of the knowledge base.\n",
    "\n",
    "Note that after 100 epochs the test accuracy is around 1. This shows the power of LTN in learning\n",
    "the multi-class multi-label classification task only using the satisfaction of a knowledge base as an objective.\n",
    "\n",
    "At the beginning of the training, the truth values of $\\phi_1$, $\\phi_2$, and $\\phi_3$ are non-informative. Instead, during\n",
    "training, one can see a trend towards the satisfaction of $\\phi_1$, and an opposite trend for $\\phi_2$ and $\\phi_3$, as expected.\n",
    "This shows the ability of LTN to query and reason on never-seen formulas.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}